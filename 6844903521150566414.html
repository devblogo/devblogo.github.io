<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="referrer" content="never">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Python,">





  <link rel="alternate" href="atom.xml" title="开发者博客 – IT技术 尽在开发者博客" type="application/atom+xml">






<meta name="description" content="&amp;#x672C;&amp;#x6587;&amp;#x4ECE;&amp;#x6570;&amp;#x636E;&amp;#x9884;&amp;#x5904;&amp;#x7406;&amp;#x5F00;&amp;#x59CB;&amp;#x8BE6;&amp;#x7EC6;&amp;#x5730;&amp;#x63CF;&amp;#x8FF0;&amp;#x4E86;&amp;#x5982;&amp;#x4F55;&amp;#x4F7F;&amp;#x7528; VGG &amp;#x548C;&amp;#x5FAA;&amp;#x73AF;&amp;#x795E;&amp;#">
<meta name="keywords" content="Python">
<meta property="og:type" content="article">
<meta property="og:title" content="从头开始在Python中开发深度学习字幕生成模型">
<meta property="og:url" content="https://dev.newban.cn/6844903521150566414.html">
<meta property="og:site_name" content="开发者博客 – IT技术 尽在开发者博客">
<meta property="og:description" content="&amp;#x672C;&amp;#x6587;&amp;#x4ECE;&amp;#x6570;&amp;#x636E;&amp;#x9884;&amp;#x5904;&amp;#x7406;&amp;#x5F00;&amp;#x59CB;&amp;#x8BE6;&amp;#x7EC6;&amp;#x5730;&amp;#x63CF;&amp;#x8FF0;&amp;#x4E86;&amp;#x5982;&amp;#x4F55;&amp;#x4F7F;&amp;#x7528; VGG &amp;#x548C;&amp;#x5FAA;&amp;#x73AF;&amp;#x795E;&amp;#">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/6a83dddd3041f6b61c914da9012ca48e9a1f0dace1f95c89732a05d20c0bf631">
<meta property="og:image" content="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/937c3700b1e6a62adc72e1ce3f87c1a34786fdd6a5fc1ae559cf9a206ba7483a">
<meta property="og:image" content="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/073fa794e019060436dfb976fb5e1c2c73469fee0071901f1d7c871acf43e4b5">
<meta property="og:image" content="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/e2660351b2541a03b38b86b84fb9436710408d5333c8d898caba78970ecd1f7e">
<meta property="og:updated_time" content="2024-04-28T03:16:06.834Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从头开始在Python中开发深度学习字幕生成模型">
<meta name="twitter:description" content="&amp;#x672C;&amp;#x6587;&amp;#x4ECE;&amp;#x6570;&amp;#x636E;&amp;#x9884;&amp;#x5904;&amp;#x7406;&amp;#x5F00;&amp;#x59CB;&amp;#x8BE6;&amp;#x7EC6;&amp;#x5730;&amp;#x63CF;&amp;#x8FF0;&amp;#x4E86;&amp;#x5982;&amp;#x4F55;&amp;#x4F7F;&amp;#x7528; VGG &amp;#x548C;&amp;#x5FAA;&amp;#x73AF;&amp;#x795E;&amp;#">
<meta name="twitter:image" content="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/6a83dddd3041f6b61c914da9012ca48e9a1f0dace1f95c89732a05d20c0bf631">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>
<!--谷歌广告验证代码-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-2626449904708114",
          enable_page_level_ads: true
     });
</script>
<!--谷歌广告验证代码-->



  <link rel="canonical" href="https://dev.newban.cn/6844903521150566414.html">





  <title>从头开始在Python中开发深度学习字幕生成模型 | 开发者博客 – IT技术 尽在开发者博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">开发者博客 – IT技术 尽在开发者博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">开发者博客 – 科技是第一生产力</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="http://dev.newban.cn/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://dev.newban.cn/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://dev.newban.cn">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="开发者博客">
      <meta itemprop="description" content>
      <meta itemprop="image" content="images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="开发者博客 – IT技术 尽在开发者博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">从头开始在Python中开发深度学习字幕生成模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-12T16:07:19+08:00">
                2017-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>&#x672C;&#x6587;&#x4ECE;&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;&#x5F00;&#x59CB;&#x8BE6;&#x7EC6;&#x5730;&#x63CF;&#x8FF0;&#x4E86;&#x5982;&#x4F55;&#x4F7F;&#x7528; VGG &#x548C;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6784;&#x5EFA;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x7CFB;&#x7EDF;&#xFF0C;&#x5BF9;&#x8BFB;&#x8005;&#x4F7F;&#x7528; Keras &#x548C; TensorFlow &#x7406;&#x89E3;&#x4E0E;&#x5B9E;&#x73B0;&#x81EA;&#x52A8;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x5F88;&#x6709;&#x5E2E;&#x52A9;&#x3002;&#x672C;&#x6587;&#x7684;&#x4EE3;&#x7801;&#x90FD;&#x6709;&#x89E3;&#x91CA;&#xFF0C;&#x975E;&#x5E38;&#x9002;&#x5408;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x4EFB;&#x52A1;&#x7684;&#x5165;&#x95E8;&#x8BFB;&#x8005;&#x8BE6;&#x7EC6;&#x4E86;&#x89E3;&#x8FD9;&#x4E00;&#x8FC7;&#x7A0B;&#x3002;</p>
</blockquote>
<p>&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x6311;&#x6218;&#x6027;&#x7684;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x95EE;&#x9898;&#xFF0C;&#x6D89;&#x53CA;&#x4E3A;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x3002;</p>
<p>&#x5B57;&#x5E55;&#x751F;&#x6210;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x6311;&#x6218;&#x6027;&#x7684;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x95EE;&#x9898;&#xFF0C;&#x6D89;&#x53CA;&#x4E3A;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x3002;</p>
<p>&#x4E00;&#x822C;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6216;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x9700;&#x8981;&#x4F7F;&#x7528;&#x8BA1;&#x7B97;&#x673A;&#x89C6;&#x89C9;&#x65B9;&#x6CD5;&#x6765;&#x4E86;&#x89E3;&#x56FE;&#x50CF;&#x5185;&#x5BB9;&#xFF0C;&#x4E5F;&#x9700;&#x8981;&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x6A21;&#x578B;&#x5C06;&#x5BF9;&#x56FE;&#x50CF;&#x7684;&#x7406;&#x89E3;&#x8F6C;&#x6362;&#x6210;&#x6B63;&#x786E;&#x987A;&#x5E8F;&#x7684;&#x6587;&#x5B57;&#x3002;&#x8FD1;&#x671F;&#xFF0C;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;&#x5728;&#x8BE5;&#x95EE;&#x9898;&#x7684;&#x591A;&#x4E2A;&#x793A;&#x4F8B;&#x4E0A;&#x83B7;&#x5F97;&#x4E86;&#x9876;&#x5C16;&#x7ED3;&#x679C;&#x3002;</p>
<p>&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;&#x5728;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x95EE;&#x9898;&#x4E0A;&#x5C55;&#x73B0;&#x4E86;&#x9876;&#x5C16;&#x7684;&#x7ED3;&#x679C;&#x3002;&#x8FD9;&#x4E9B;&#x65B9;&#x6CD5;&#x6700;&#x4EE4;&#x4EBA;&#x5370;&#x8C61;&#x6DF1;&#x523B;&#x7684;&#x5730;&#x65B9;&#xFF1A;&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x56FE;&#x50CF;&#xFF0C;&#x6211;&#x4EEC;&#x65E0;&#x9700;&#x590D;&#x6742;&#x7684;&#x6570;&#x636E;&#x51C6;&#x5907;&#x548C;&#x7279;&#x6B8A;&#x8BBE;&#x8BA1;&#x7684;&#x6D41;&#x7A0B;&#xFF0C;&#x5C31;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x7AEF;&#x5230;&#x7AEF;&#x7684;&#x65B9;&#x5F0F;&#x9884;&#x6D4B;&#x5B57;&#x5E55;&#x3002;</p>
<p>&#x672C;&#x6559;&#x7A0B;&#x5C06;&#x4ECB;&#x7ECD;&#x5982;&#x4F55;&#x4ECE;&#x5934;&#x5F00;&#x53D1;&#x80FD;&#x751F;&#x6210;&#x56FE;&#x50CF;&#x5B57;&#x5E55;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x5B8C;&#x6210;&#x672C;&#x6559;&#x7A0B;&#xFF0C;&#x4F60;&#x5C06;&#x5B66;&#x4F1A;&#xFF1A;</p>
<ul>
<li>&#x5982;&#x4F55;&#x4E3A;&#x8BAD;&#x7EC3;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#x51C6;&#x5907;&#x56FE;&#x50CF;&#x548C;&#x6587;&#x672C;&#x6570;&#x636E;&#x3002;</li>
<li>&#x5982;&#x4F55;&#x8BBE;&#x8BA1;&#x548C;&#x8BAD;&#x7EC3;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x6A21;&#x578B;&#x3002;</li>
<li>&#x5982;&#x4F55;&#x8BC4;&#x4F30;&#x4E00;&#x4E2A;&#x8BAD;&#x7EC3;&#x540E;&#x7684;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x6A21;&#x578B;&#xFF0C;&#x5E76;&#x4F7F;&#x7528;&#x5B83;&#x4E3A;&#x5168;&#x65B0;&#x7684;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x5B57;&#x5E55;&#x3002;</li>
</ul>
<p><img src="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/6a83dddd3041f6b61c914da9012ca48e9a1f0dace1f95c89732a05d20c0bf631" alt>  </p>
<p><strong>&#x6559;&#x7A0B;&#x6982;&#x89C8;</strong></p>
<p>&#x8BE5;&#x6559;&#x7A0B;&#x5171;&#x5206;&#x4E3A; 6 &#x90E8;&#x5206;&#xFF1A;</p>
<ol>
<li>&#x56FE;&#x50CF;&#x548C;&#x5B57;&#x5E55;&#x6570;&#x636E;&#x96C6;</li>
</ol>
<ol start="2">
<li>&#x51C6;&#x5907;&#x56FE;&#x50CF;&#x6570;&#x636E;</li>
</ol>
<ol start="3">
<li>&#x51C6;&#x5907;&#x6587;&#x672C;&#x6570;&#x636E;</li>
</ol>
<ol start="4">
<li>&#x5F00;&#x53D1;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;</li>
</ol>
<ol start="5">
<li>&#x8BC4;&#x4F30;&#x6A21;&#x578B;</li>
</ol>
<ol start="6">
<li>&#x751F;&#x6210;&#x65B0;&#x7684;&#x5B57;&#x5E55;</li>
</ol>
<p><strong>Python &#x73AF;&#x5883;</strong></p>
<p>&#x672C;&#x6559;&#x7A0B;&#x5047;&#x8BBE;&#x4F60;&#x5DF2;&#x7ECF;&#x5B89;&#x88C5;&#x4E86; Python SciPy &#x73AF;&#x5883;&#xFF0C;&#x8BE5;&#x73AF;&#x5883;&#x5B8C;&#x7F8E;&#x9002;&#x5408; Python 3&#x3002;&#x4F60;&#x5FC5;&#x987B;&#x5B89;&#x88C5; Keras&#xFF08;2.0 &#x7248;&#x672C;&#x6216;&#x66F4;&#x9AD8;&#xFF09;&#xFF0C;TensorFlow &#x6216; Theano &#x540E;&#x7AEF;&#x3002;&#x672C;&#x6559;&#x7A0B;&#x8FD8;&#x5047;&#x8BBE;&#x4F60;&#x5DF2;&#x7ECF;&#x5B89;&#x88C5;&#x4E86; scikit-learn&#x3001;Pandas&#x3001;NumPy &#x548C; Matplotlib &#x7B49;&#x79D1;&#x5B66;&#x8BA1;&#x7B97;&#x4E0E;&#x7ED8;&#x56FE;&#x8F6F;&#x4EF6;&#x5E93;&#x3002;</p>
<p>&#x6211;&#x63A8;&#x8350;&#x5728; GPU &#x7CFB;&#x7EDF;&#x4E0A;&#x8FD0;&#x884C;&#x4EE3;&#x7801;&#x3002;&#x4F60;&#x53EF;&#x4EE5;&#x5728; Amazon Web Services &#x4E0A;&#x7528;&#x5EC9;&#x4EF7;&#x7684;&#x65B9;&#x5F0F;&#x83B7;&#x53D6; GPU&#xFF1A;<a href="/external_links/c0b81c3e330e2c9da6df2ee3be52971b.html" target="blank" rel="noopener">&#x5982;&#x4F55;&#x5728; AWS GPU &#x4E0A;&#x8FD0;&#x884C; Jupyter noterbook</a>&#xFF1F;</p>
<p><strong>&#x56FE;&#x50CF;&#x548C;&#x5B57;&#x5E55;&#x6570;&#x636E;&#x96C6;</strong></p>
<p>&#x56FE;&#x50CF;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x53EF;&#x4F7F;&#x7528;&#x7684;&#x4F18;&#x79C0;&#x6570;&#x636E;&#x96C6;&#x6709; Flickr8K &#x6570;&#x636E;&#x96C6;&#x3002;&#x539F;&#x56E0;&#x5728;&#x4E8E;&#x5B83;&#x903C;&#x771F;&#x4E14;&#x76F8;&#x5BF9;&#x8F83;&#x5C0F;&#xFF0C;&#x5373;&#x4F7F;&#x4F60;&#x7684;&#x5DE5;&#x4F5C;&#x7AD9;&#x4F7F;&#x7528;&#x7684;&#x662F; CPU &#x4E5F;&#x53EF;&#x4EE5;&#x4E0B;&#x8F7D;&#x5B83;&#xFF0C;&#x5E76;&#x7528;&#x4E8E;&#x6784;&#x5EFA;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x5BF9;&#x8BE5;&#x6570;&#x636E;&#x96C6;&#x7684;&#x660E;&#x786E;&#x63CF;&#x8FF0;&#x89C1; 2013 &#x5E74;&#x7684;&#x8BBA;&#x6587;&#x300A;Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics&#x300B;&#x3002;</p>
<p>&#x4F5C;&#x8005;&#x5BF9;&#x8BE5;&#x6570;&#x636E;&#x96C6;&#x7684;&#x63CF;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<blockquote>
<p>&#x6211;&#x4EEC;&#x4ECB;&#x7ECD;&#x4E86;&#x4E00;&#x79CD;&#x7528;&#x4E8E;&#x57FA;&#x4E8E;&#x53E5;&#x5B50;&#x7684;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x548C;&#x641C;&#x7D22;&#x7684;&#x65B0;&#x578B;&#x57FA;&#x51C6;&#x96C6;&#x5408;&#xFF0C;&#x5305;&#x62EC; 8000 &#x5F20;&#x56FE;&#x50CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x56FE;&#x50CF;&#x6709;&#x4E94;&#x4E2A;&#x4E0D;&#x540C;&#x7684;&#x5B57;&#x5E55;&#x63CF;&#x8FF0;&#x5BF9;&#x7A81;&#x51FA;&#x5B9E;&#x4F53;&#x548C;&#x4E8B;&#x4EF6;&#x63D0;&#x4F9B;&#x6E05;&#x6670;&#x63CF;&#x8FF0;&#x3002;</p>
<p>&#x56FE;&#x50CF;&#x9009;&#x81EA;&#x516D;&#x4E2A;&#x4E0D;&#x540C;&#x7684; Flickr &#x7EC4;&#xFF0C;&#x5F80;&#x5F80;&#x4E0D;&#x5305;&#x542B;&#x540D;&#x4EBA;&#x6216;&#x6709;&#x540D;&#x7684;&#x5730;&#x70B9;&#xFF0C;&#x800C;&#x662F;&#x624B;&#x52A8;&#x9009;&#x62E9;&#x591A;&#x79CD;&#x573A;&#x666F;&#x548C;&#x60C5;&#x5F62;&#x3002;</p>
</blockquote>
<p>&#x8BE5;&#x6570;&#x636E;&#x96C6;&#x53EF;&#x514D;&#x8D39;&#x83B7;&#x53D6;&#x3002;&#x4F60;&#x5FC5;&#x987B;&#x586B;&#x5199;&#x4E00;&#x4EFD;&#x7533;&#x8BF7;&#x8868;&#xFF0C;&#x7136;&#x540E;&#x5C31;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x7535;&#x5B50;&#x90AE;&#x7BB1;&#x6536;&#x5230;&#x6570;&#x636E;&#x96C6;&#x3002;&#x7533;&#x8BF7;&#x8868;&#x94FE;&#x63A5;&#xFF1A;<a href="/external_links/833c2d953be845e3a6c2fc84915fe9e8.html" target="blank" rel="noopener">https://illinois.edu/fb/sec/1713398&#x3002;</a></p>
<p>&#x5F88;&#x5FEB;&#xFF0C;&#x4F60;&#x4F1A;&#x6536;&#x5230;&#x7535;&#x5B50;&#x90AE;&#x4EF6;&#xFF0C;&#x5305;&#x542B;&#x4EE5;&#x4E0B;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;&#x7684;&#x94FE;&#x63A5;&#xFF1A;</p>
<ul>
<li>Flickr8k_Dataset.zip&#xFF08;1 Gigabyte&#xFF09;&#x5305;&#x542B;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x3002;</li>
<li>Flickr8k_text.zip&#xFF08;2.2 Megabytes&#xFF09;&#x5305;&#x542B;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x3002;</li>
</ul>
<p>&#x4E0B;&#x8F7D;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x5E76;&#x5728;&#x5F53;&#x524D;&#x5DE5;&#x4F5C;&#x6587;&#x4EF6;&#x5939;&#x91CC;&#x8FDB;&#x884C;&#x89E3;&#x538B;&#x7F29;&#x3002;&#x4F60;&#x5C06;&#x5F97;&#x5230;&#x4E24;&#x4E2A;&#x76EE;&#x5F55;&#xFF1A;</p>
<ul>
<li>Flicker8k_Dataset&#xFF1A;&#x5305;&#x542B; 8092 &#x5F20; JPEG &#x683C;&#x5F0F;&#x56FE;&#x50CF;&#x3002;</li>
<li>Flickr8k_text&#xFF1A;&#x5305;&#x542B;&#x5927;&#x91CF;&#x4E0D;&#x540C;&#x6765;&#x6E90;&#x7684;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6587;&#x4EF6;&#x3002;</li>
</ul>
<p>&#x8BE5;&#x6570;&#x636E;&#x96C6;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x9884;&#x5236;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#xFF08;6000 &#x5F20;&#x56FE;&#x50CF;&#xFF09;&#x3001;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#xFF08;1000 &#x5F20;&#x56FE;&#x50CF;&#xFF09;&#x548C;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#xFF08;1000 &#x5F20;&#x56FE;&#x50CF;&#xFF09;&#x3002;</p>
<p>&#x7528;&#x4E8E;&#x8BC4;&#x4F30;&#x6A21;&#x578B;&#x6280;&#x80FD;&#x7684;&#x4E00;&#x4E2A;&#x6307;&#x6807;&#x662F; BLEU &#x503C;&#x3002;&#x5BF9;&#x4E8E;&#x63A8;&#x65AD;&#xFF0C;&#x4E0B;&#x9762;&#x662F;&#x4E00;&#x4E9B;&#x7CBE;&#x5DE7;&#x7684;&#x6A21;&#x578B;&#x5728;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x8FDB;&#x884C;&#x8BC4;&#x4F30;&#x65F6;&#x83B7;&#x5F97;&#x7684;&#x5927;&#x6982; BLEU &#x503C;&#xFF08;&#x6765;&#x6E90;&#xFF1A;2017 &#x5E74;&#x8BBA;&#x6587;&#x300A;Where to put the Image in an Image Caption Generator&#x300B;&#xFF09;&#xFF1A;</p>
<ul>
<li>BLEU-1: 0.401 to 0.578.</li>
<li>BLEU-2: 0.176 to 0.390.</li>
<li>BLEU-3: 0.099 to 0.260.</li>
<li>BLEU-4: 0.059 to 0.170.</li>
</ul>
<p>&#x7A0D;&#x540E;&#x5728;&#x8BC4;&#x4F30;&#x6A21;&#x578B;&#x90E8;&#x5206;&#x5C06;&#x8BE6;&#x7EC6;&#x4ECB;&#x7ECD; BLEU &#x503C;&#x3002;&#x4E0B;&#x9762;&#xFF0C;&#x6211;&#x4EEC;&#x6765;&#x770B;&#x4E00;&#x4E0B;&#x5982;&#x4F55;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x3002;</p>
<p><strong>&#x51C6;&#x5907;&#x56FE;&#x50CF;&#x6570;&#x636E;</strong></p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x89E3;&#x6790;&#x56FE;&#x50CF;&#x5185;&#x5BB9;&#xFF0C;&#x4E14;&#x76EE;&#x524D;&#x6709;&#x5F88;&#x591A;&#x53EF;&#x9009;&#x6A21;&#x578B;&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528; Oxford Visual Geometry Group &#x6216; VGG&#xFF08;&#x8BE5;&#x6A21;&#x578B;&#x8D62;&#x5F97;&#x4E86; 2014 &#x5E74; ImageNet &#x7ADE;&#x8D5B;&#x51A0;&#x519B;&#xFF09;&#x3002;</p>
<p>Keras &#x53EF;&#x76F4;&#x63A5;&#x63D0;&#x4F9B;&#x8BE5;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x3002;&#x6CE8;&#x610F;&#xFF0C;&#x7B2C;&#x4E00;&#x6B21;&#x4F7F;&#x7528;&#x8BE5;&#x6A21;&#x578B;&#x65F6;&#xFF0C;Keras &#x5C06;&#x4ECE;&#x4E92;&#x8054;&#x7F51;&#x4E0A;&#x4E0B;&#x8F7D;&#x6A21;&#x578B;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x6982; 500Megabytes&#x3002;&#x8FD9;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x4E00;&#x6BB5;&#x65F6;&#x95F4;&#xFF08;&#x65F6;&#x95F4;&#x957F;&#x5EA6;&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x7684;&#x7F51;&#x7EDC;&#x8FDE;&#x63A5;&#xFF09;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5C06;&#x8BE5;&#x6A21;&#x578B;&#x4F5C;&#x4E3A;&#x66F4;&#x5927;&#x7684;&#x56FE;&#x50CF;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x6A21;&#x578B;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x3002;&#x95EE;&#x9898;&#x5728;&#x4E8E;&#x6A21;&#x578B;&#x592A;&#x5927;&#xFF0C;&#x6BCF;&#x6B21;&#x6211;&#x4EEC;&#x60F3;&#x6D4B;&#x8BD5;&#x65B0;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x914D;&#x7F6E;&#xFF08;&#x4E0B;&#x884C;&#xFF09;&#x65F6;&#x5728;&#x8BE5;&#x7F51;&#x7EDC;&#x4E2D;&#x8FD0;&#x884C;&#x6BCF;&#x5F20;&#x56FE;&#x50CF;&#x975E;&#x5E38;&#x5197;&#x4F59;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x5BF9;&#x300C;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x300D;&#x8FDB;&#x884C;&#x9884;&#x8BA1;&#x7B97;&#xFF0C;&#x5E76;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#x4E2D;&#x3002;&#x7136;&#x540E;&#x52A0;&#x8F7D;&#x8FD9;&#x4E9B;&#x7279;&#x5F81;&#xFF0C;&#x5C06;&#x5176;&#x9988;&#x9001;&#x81F3;&#x6A21;&#x578B;&#x4E2D;&#x4F5C;&#x4E3A;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x7684;&#x63CF;&#x8FF0;&#x3002;&#x5728;&#x5B8C;&#x6574;&#x7684; VGG &#x6A21;&#x578B;&#x4E2D;&#x8FD0;&#x884C;&#x56FE;&#x50CF;&#x4E5F;&#x662F;&#x8FD9;&#x6837;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x63D0;&#x524D;&#x8FD0;&#x884C;&#x8BE5;&#x6B65;&#x9AA4;&#x3002;</p>
<p>&#x4F18;&#x5316;&#x53EF;&#x4EE5;&#x52A0;&#x5FEB;&#x6A21;&#x578B;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#xFF0C;&#x6D88;&#x8017;&#x66F4;&#x5C11;&#x5185;&#x5B58;&#x3002;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; VGG class &#x5728; Keras &#x4E2D;&#x8FD0;&#x884C; VGG &#x6A21;&#x578B;&#x3002;&#x6211;&#x4EEC;&#x5C06;&#x79FB;&#x9664;&#x52A0;&#x8F7D;&#x6A21;&#x578B;&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x56E0;&#x4E3A;&#x8BE5;&#x5C42;&#x7528;&#x4E8E;&#x9884;&#x6D4B;&#x56FE;&#x50CF;&#x7684;&#x5206;&#x7C7B;&#x3002;&#x6211;&#x4EEC;&#x5BF9;&#x56FE;&#x50CF;&#x5206;&#x7C7B;&#x4E0D;&#x611F;&#x5174;&#x8DA3;&#xFF0C;&#x6211;&#x4EEC;&#x611F;&#x5174;&#x8DA3;&#x7684;&#x662F;&#x5206;&#x7C7B;&#x4E4B;&#x524D;&#x56FE;&#x50CF;&#x7684;&#x5185;&#x90E8;&#x8868;&#x5F81;&#x3002;&#x8FD9;&#x4E9B;&#x5C31;&#x662F;&#x6A21;&#x578B;&#x4ECE;&#x56FE;&#x50CF;&#x4E2D;&#x63D0;&#x53D6;&#x51FA;&#x7684;&#x300C;&#x7279;&#x5F81;&#x300D;&#x3002;</p>
<p>Keras &#x8FD8;&#x63D0;&#x4F9B;&#x5DE5;&#x5177;&#x5C06;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x6539;&#x9020;&#x6210;&#x6A21;&#x578B;&#x7684;&#x504F;&#x597D;&#x5927;&#x5C0F;&#xFF08;&#x5982; 3 &#x901A;&#x9053; 224 x 224 &#x50CF;&#x7D20;&#x56FE;&#x50CF;&#xFF09;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x662F; extract_features() &#x51FD;&#x6570;&#xFF0C;&#x5373;&#x7ED9;&#x51FA;&#x4E00;&#x4E2A;&#x76EE;&#x5F55;&#x540D;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x52A0;&#x8F7D;&#x6BCF;&#x4E2A;&#x56FE;&#x50CF;&#x3001;&#x4E3A; VGG &#x51C6;&#x5907;&#x56FE;&#x50CF;&#x6570;&#x636E;&#xFF0C;&#x5E76;&#x4ECE; VGG &#x6A21;&#x578B;&#x4E2D;&#x6536;&#x96C6;&#x9884;&#x6D4B;&#x5230;&#x7684;&#x7279;&#x5F81;&#x3002;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x662F;&#x5305;&#x542B; 4096 &#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5411;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#xFF08;identifier&#xFF09;&#x8BCD;&#x5178;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# extract features from each photo in the directory</span><br><span class="line">def extract_features(directory):</span><br><span class="line">	# load the model</span><br><span class="line">	model = VGG16()</span><br><span class="line">	# re-structure the model</span><br><span class="line">	model.layers.pop()</span><br><span class="line">	model = Model(inputs=model.inputs, outputs=model.layers[-1].output)</span><br><span class="line">	# summarize</span><br><span class="line">	print(model.summary())</span><br><span class="line">	# extract features from each photo</span><br><span class="line">	features = dict()</span><br><span class="line">	for name in listdir(directory):</span><br><span class="line">		# load an image from file</span><br><span class="line">		filename = directory + &apos;/&apos; + name</span><br><span class="line">		image = load_img(filename, target_size=(224, 224))</span><br><span class="line">		# convert the image pixels to a numpy array</span><br><span class="line">		image = img_to_array(image)</span><br><span class="line">		# reshape data for the model</span><br><span class="line">		image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line">		# prepare the image for the VGG model</span><br><span class="line">		image = preprocess_input(image)</span><br><span class="line">		# get features</span><br><span class="line">		feature = model.predict(image, verbose=0)</span><br><span class="line">		# get image id</span><br><span class="line">		image_id = name.split(&apos;.&apos;)[0]</span><br><span class="line">		# store feature</span><br><span class="line">		features[image_id] = feature</span><br><span class="line">		print(&apos;&gt;%s&apos; % name)</span><br><span class="line">	return features</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x8C03;&#x7528;&#x8BE5;&#x51FD;&#x6570;&#x4E3A;&#x6A21;&#x578B;&#x6D4B;&#x8BD5;&#x51C6;&#x5907;&#x56FE;&#x50CF;&#x6570;&#x636E;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x8BCD;&#x5178;&#x4FDD;&#x5B58;&#x81F3; features.pkl &#x6587;&#x4EF6;&#x3002;</p>
<p>&#x5B8C;&#x6574;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;from os import listdir</span><br><span class="line">from pickle import dump</span><br><span class="line">from keras.applications.vgg16 import VGG16</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.applications.vgg16 import preprocess_input</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># extract features from each photo in the directory</span><br><span class="line">def extract_features(directory):</span><br><span class="line">	# load the model</span><br><span class="line">	model = VGG16()</span><br><span class="line">	# re-structure the model</span><br><span class="line">	model.layers.pop()</span><br><span class="line">	model = Model(inputs=model.inputs, outputs=model.layers[-1].output)</span><br><span class="line">	# summarize</span><br><span class="line">	print(model.summary())</span><br><span class="line">	# extract features from each photo</span><br><span class="line">	features = dict()</span><br><span class="line">	for name in listdir(directory):</span><br><span class="line">		# load an image from file</span><br><span class="line">		filename = directory + &apos;/&apos; + name</span><br><span class="line">		image = load_img(filename, target_size=(224, 224))</span><br><span class="line">		# convert the image pixels to a numpy array</span><br><span class="line">		image = img_to_array(image)</span><br><span class="line">		# reshape data for the model</span><br><span class="line">		image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line">		# prepare the image for the VGG model</span><br><span class="line">		image = preprocess_input(image)</span><br><span class="line">		# get features</span><br><span class="line">		feature = model.predict(image, verbose=0)</span><br><span class="line">		# get image id</span><br><span class="line">		image_id = name.split(&apos;.&apos;)[0]</span><br><span class="line">		# store feature</span><br><span class="line">		features[image_id] = feature</span><br><span class="line">		print(&apos;&gt;%s&apos; % name)</span><br><span class="line">	return features</span><br><span class="line"></span><br><span class="line"># extract features from all images</span><br><span class="line">directory = &apos;Flicker8k_Dataset&apos;</span><br><span class="line">features = extract_features(directory)</span><br><span class="line">print(&apos;Extracted Features: %d&apos; % len(features))</span><br><span class="line"># save to file</span><br><span class="line">dump(features, open(&apos;features.pkl&apos;, &apos;wb&apos;))</span><br></pre></td></tr></table></figure>

<p>&#x8FD0;&#x884C;&#x8BE5;&#x6570;&#x636E;&#x51C6;&#x5907;&#x6B65;&#x9AA4;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x4E00;&#x70B9;&#x65F6;&#x95F4;&#xFF0C;&#x65F6;&#x95F4;&#x957F;&#x5EA6;&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x7684;&#x786C;&#x4EF6;&#xFF0C;&#x5E26;&#x6709; CPU &#x7684;&#x73B0;&#x4EE3;&#x5DE5;&#x4F5C;&#x7AD9;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x4E00;&#x4E2A;&#x5C0F;&#x65F6;&#x3002;</p>
<p>&#x8FD0;&#x884C;&#x7ED3;&#x675F;&#x65F6;&#xFF0C;&#x63D0;&#x53D6;&#x51FA;&#x7684;&#x7279;&#x5F81;&#x5C06;&#x5B58;&#x50A8;&#x5728; features.pkl &#x6587;&#x4EF6;&#x4E2D;&#x4EE5;&#x5907;&#x540E;&#x7528;&#x3002;&#x8BE5;&#x6587;&#x4EF6;&#x5927;&#x6982; 127 Megabytes &#x5927;&#x5C0F;&#x3002;</p>
<p><strong>&#x51C6;&#x5907;&#x6587;&#x672C;&#x6570;&#x636E;</strong></p>
<p>&#x8BE5;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x6BCF;&#x4E2A;&#x56FE;&#x50CF;&#x6709;&#x591A;&#x4E2A;&#x63CF;&#x8FF0;&#xFF0C;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x9700;&#x8981;&#x8FDB;&#x884C;&#x6700;&#x4F4E;&#x9650;&#x5EA6;&#x7684;&#x6E05;&#x6D17;&#x3002;&#x9996;&#x5148;&#xFF0C;&#x52A0;&#x8F7D;&#x5305;&#x542B;&#x6240;&#x6709;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x7684;&#x6587;&#x4EF6;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line">filename = &apos;Flickr8k_text/Flickr8k.token.txt&apos;</span><br><span class="line"># load descriptions</span><br><span class="line">doc = load_doc(filename)</span><br></pre></td></tr></table></figure>

<p>&#x6BCF;&#x4E2A;&#x56FE;&#x50CF;&#x6709;&#x4E00;&#x4E2A;&#x72EC;&#x6709;&#x7684;&#x6807;&#x8BC6;&#x7B26;&#xFF0C;&#x8BE5;&#x6807;&#x8BC6;&#x7B26;&#x51FA;&#x73B0;&#x5728;&#x6587;&#x4EF6;&#x540D;&#x548C;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x6587;&#x4EF6;&#x4E2D;&#x3002;</p>
<p>&#x63A5;&#x4E0B;&#x6765;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x9010;&#x6B65;&#x5BF9;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x8FDB;&#x884C;&#x64CD;&#x4F5C;&#x3002;&#x4E0B;&#x9762;&#x5B9A;&#x4E49;&#x4E00;&#x4E2A; load_descriptions() &#x51FD;&#x6570;&#xFF1A;&#x7ED9;&#x51FA;&#x4E00;&#x4E2A;&#x9700;&#x8981;&#x52A0;&#x8F7D;&#x7684;&#x6587;&#x672C;&#x6587;&#x6863;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x8FD4;&#x56DE;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x8BCD;&#x5178;&#x3002;&#x6BCF;&#x4E2A;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x6620;&#x5C04;&#x5230;&#x4E00;&#x6216;&#x591A;&#x4E2A;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# extract descriptions for images</span><br><span class="line">def load_descriptions(doc):</span><br><span class="line">	mapping = dict()</span><br><span class="line">	# process lines</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		if len(line) &lt; 2:</span><br><span class="line">			continue</span><br><span class="line">		# take the first token as the image id, the rest as the description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# remove filename from image id</span><br><span class="line">		image_id = image_id.split(&apos;.&apos;)[0]</span><br><span class="line">		# convert description tokens back to string</span><br><span class="line">		image_desc = &apos; &apos;.join(image_desc)</span><br><span class="line">		# create the list if needed</span><br><span class="line">		if image_id not in mapping:</span><br><span class="line">			mapping[image_id] = list()</span><br><span class="line">		# store description</span><br><span class="line">		mapping[image_id].append(image_desc)</span><br><span class="line">	return mapping</span><br><span class="line"></span><br><span class="line"># parse descriptions</span><br><span class="line">descriptions = load_descriptions(doc)</span><br><span class="line">print(&apos;Loaded: %d &apos; % len(descriptions))</span><br></pre></td></tr></table></figure>

<p>&#x4E0B;&#x9762;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x6E05;&#x6D17;&#x63CF;&#x8FF0;&#x6587;&#x672C;&#x3002;&#x56E0;&#x4E3A;&#x63CF;&#x8FF0;&#x5DF2;&#x7ECF;&#x7ECF;&#x8FC7;&#x7B26;&#x53F7;&#x5316;&#xFF0C;&#x6240;&#x4EE5;&#x5B83;&#x5341;&#x5206;&#x6613;&#x4E8E;&#x5904;&#x7406;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x7528;&#x4EE5;&#x4E0B;&#x65B9;&#x5F0F;&#x6E05;&#x6D17;&#x6587;&#x672C;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;&#x9700;&#x8981;&#x5904;&#x7406;&#x7684;&#x8BCD;&#x6C47;&#x91CF;&#xFF1A;</p>
<ul>
<li>&#x6240;&#x6709;&#x5355;&#x8BCD;&#x5168;&#x90E8;&#x8F6C;&#x6362;&#x6210;&#x5C0F;&#x5199;&#x3002;</li>
<li>&#x79FB;&#x9664;&#x6240;&#x6709;&#x6807;&#x70B9;&#x7B26;&#x53F7;&#x3002;</li>
<li>&#x79FB;&#x9664;&#x6240;&#x6709;&#x5C11;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;&#x4E00;&#x4E2A;&#x5B57;&#x7B26;&#x7684;&#x5355;&#x8BCD;&#xFF08;&#x5982; a&#xFF09;&#x3002;</li>
<li>&#x79FB;&#x9664;&#x6240;&#x6709;&#x5E26;&#x6570;&#x5B57;&#x7684;&#x5355;&#x8BCD;&#x3002;</li>
</ul>
<p>&#x4E0B;&#x9762;&#x5B9A;&#x4E49;&#x4E86; clean_descriptions() &#x51FD;&#x6570;&#xFF1A;&#x7ED9;&#x51FA;&#x63CF;&#x8FF0;&#x7684;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x8BCD;&#x5178;&#xFF0C;&#x904D;&#x5386;&#x6BCF;&#x4E2A;&#x63CF;&#x8FF0;&#xFF0C;&#x6E05;&#x6D17;&#x6587;&#x672C;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;import string</span><br><span class="line"></span><br><span class="line">def clean_descriptions(descriptions):</span><br><span class="line">	# prepare translation table for removing punctuation</span><br><span class="line">	table = str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		for i in range(len(desc_list)):</span><br><span class="line">			desc = desc_list[i]</span><br><span class="line">			# tokenize</span><br><span class="line">			desc = desc.split()</span><br><span class="line">			# convert to lower case</span><br><span class="line">			desc = [word.lower() for word in desc]</span><br><span class="line">			# remove punctuation from each token</span><br><span class="line">			desc = [w.translate(table) for w in desc]</span><br><span class="line">			# remove hanging &apos;s&apos; and &apos;a&apos;</span><br><span class="line">			desc = [word for word in desc if len(word)&gt;1]</span><br><span class="line">			# remove tokens with numbers in them</span><br><span class="line">			desc = [word for word in desc if word.isalpha()]</span><br><span class="line">			# store as string</span><br><span class="line">			desc_list[i] =  &apos; &apos;.join(desc)</span><br><span class="line"></span><br><span class="line"># clean descriptions</span><br><span class="line">clean_descriptions(descriptions)</span><br></pre></td></tr></table></figure>

<p>&#x6E05;&#x6D17;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x603B;&#x7ED3;&#x8BCD;&#x6C47;&#x91CF;&#x3002;</p>
<p>&#x7406;&#x60F3;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x4F7F;&#x7528;&#x5C3D;&#x53EF;&#x80FD;&#x5C11;&#x7684;&#x8BCD;&#x6C47;&#x800C;&#x5F97;&#x5230;&#x5F3A;&#x5927;&#x7684;&#x8868;&#x8FBE;&#x6027;&#x3002;&#x8BCD;&#x6C47;&#x8D8A;&#x5C11;&#x5219;&#x6A21;&#x578B;&#x8D8A;&#x5C0F;&#x3001;&#x8BAD;&#x7EC3;&#x901F;&#x5EA6;&#x8D8A;&#x5FEB;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x63A8;&#x65AD;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5C06;&#x5E72;&#x51C0;&#x7684;&#x63CF;&#x8FF0;&#x8F6C;&#x6362;&#x6210;&#x4E00;&#x4E2A;&#x96C6;&#xFF0C;&#x5C06;&#x5B83;&#x7684;&#x89C4;&#x6A21;&#x6253;&#x5370;&#x51FA;&#x6765;&#xFF0C;&#x8FD9;&#x6837;&#x5C31;&#x53EF;&#x4EE5;&#x4E86;&#x89E3;&#x6211;&#x4EEC;&#x7684;&#x6570;&#x636E;&#x96C6;&#x8BCD;&#x6C47;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E86;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# convert the loaded descriptions into a vocabulary of words</span><br><span class="line">def to_vocabulary(descriptions):</span><br><span class="line">	# build a list of all description strings</span><br><span class="line">	all_desc = set()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.update(d.split()) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># summarize vocabulary</span><br><span class="line">vocabulary = to_vocabulary(descriptions)</span><br><span class="line">print(&apos;Vocabulary Size: %d&apos; % len(vocabulary))</span><br></pre></td></tr></table></figure>

<p>&#x6700;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x4FDD;&#x5B58;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x8BCD;&#x5178;&#x548C;&#x63CF;&#x8FF0;&#x81F3;&#x4E00;&#x4E2A;&#x65B0;&#x6587;&#x672C; descriptions.txt&#xFF0C;&#x8BE5;&#x6587;&#x4EF6;&#x4E2D;&#x6BCF;&#x884C;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x56FE;&#x50CF;&#x548C;&#x4E00;&#x4E2A;&#x63CF;&#x8FF0;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x6211;&#x4EEC;&#x5B9A;&#x4E49;&#x4E86; save_doc() &#x51FD;&#x6570;&#xFF0C;&#x5373;&#x7ED9;&#x51FA;&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x6807;&#x8BC6;&#x7B26;&#x548C;&#x63CF;&#x8FF0;&#x4E4B;&#x95F4;&#x6620;&#x5C04;&#x7684;&#x8BCD;&#x5178;&#x548C;&#x6587;&#x4EF6;&#x540D;&#xFF0C;&#x5C06;&#x8BE5;&#x6620;&#x5C04;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#x4E2D;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# save descriptions to file, one per line</span><br><span class="line">def save_descriptions(descriptions, filename):</span><br><span class="line">	lines = list()</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		for desc in desc_list:</span><br><span class="line">			lines.append(key + &apos; &apos; + desc)</span><br><span class="line">	data = &apos;\n&apos;.join(lines)</span><br><span class="line">	file = open(filename, &apos;w&apos;)</span><br><span class="line">	file.write(data)</span><br><span class="line">	file.close()</span><br><span class="line"></span><br><span class="line"># save descriptions</span><br><span class="line">save_doc(descriptions, &apos;descriptions.txt&apos;)</span><br></pre></td></tr></table></figure>

<p>&#x6C47;&#x603B;&#x8D77;&#x6765;&#xFF0C;&#x5B8C;&#x6574;&#x7684;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#x6240;&#x793A;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;import string</span><br><span class="line"></span><br><span class="line"># load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># extract descriptions for images</span><br><span class="line">def load_descriptions(doc):</span><br><span class="line">	mapping = dict()</span><br><span class="line">	# process lines</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		if len(line) &lt; 2:</span><br><span class="line">			continue</span><br><span class="line">		# take the first token as the image id, the rest as the description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# remove filename from image id</span><br><span class="line">		image_id = image_id.split(&apos;.&apos;)[0]</span><br><span class="line">		# convert description tokens back to string</span><br><span class="line">		image_desc = &apos; &apos;.join(image_desc)</span><br><span class="line">		# create the list if needed</span><br><span class="line">		if image_id not in mapping:</span><br><span class="line">			mapping[image_id] = list()</span><br><span class="line">		# store description</span><br><span class="line">		mapping[image_id].append(image_desc)</span><br><span class="line">	return mapping</span><br><span class="line"></span><br><span class="line">def clean_descriptions(descriptions):</span><br><span class="line">	# prepare translation table for removing punctuation</span><br><span class="line">	table = str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		for i in range(len(desc_list)):</span><br><span class="line">			desc = desc_list[i]</span><br><span class="line">			# tokenize</span><br><span class="line">			desc = desc.split()</span><br><span class="line">			# convert to lower case</span><br><span class="line">			desc = [word.lower() for word in desc]</span><br><span class="line">			# remove punctuation from each token</span><br><span class="line">			desc = [w.translate(table) for w in desc]</span><br><span class="line">			# remove hanging &apos;s&apos; and &apos;a&apos;</span><br><span class="line">			desc = [word for word in desc if len(word)&gt;1]</span><br><span class="line">			# remove tokens with numbers in them</span><br><span class="line">			desc = [word for word in desc if word.isalpha()]</span><br><span class="line">			# store as string</span><br><span class="line">			desc_list[i] =  &apos; &apos;.join(desc)</span><br><span class="line"></span><br><span class="line"># convert the loaded descriptions into a vocabulary of words</span><br><span class="line">def to_vocabulary(descriptions):</span><br><span class="line">	# build a list of all description strings</span><br><span class="line">	all_desc = set()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.update(d.split()) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># save descriptions to file, one per line</span><br><span class="line">def save_descriptions(descriptions, filename):</span><br><span class="line">	lines = list()</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		for desc in desc_list:</span><br><span class="line">			lines.append(key + &apos; &apos; + desc)</span><br><span class="line">	data = &apos;\n&apos;.join(lines)</span><br><span class="line">	file = open(filename, &apos;w&apos;)</span><br><span class="line">	file.write(data)</span><br><span class="line">	file.close()</span><br><span class="line"></span><br><span class="line">filename = &apos;Flickr8k_text/Flickr8k.token.txt&apos;</span><br><span class="line"># load descriptions</span><br><span class="line">doc = load_doc(filename)</span><br><span class="line"># parse descriptions</span><br><span class="line">descriptions = load_descriptions(doc)</span><br><span class="line">print(&apos;Loaded: %d &apos; % len(descriptions))</span><br><span class="line"># clean descriptions</span><br><span class="line">clean_descriptions(descriptions)</span><br><span class="line"># summarize vocabulary</span><br><span class="line">vocabulary = to_vocabulary(descriptions)</span><br><span class="line">print(&apos;Vocabulary Size: %d&apos; % len(vocabulary))</span><br><span class="line"># save to file</span><br><span class="line">save_descriptions(descriptions, &apos;descriptions.txt&apos;)</span><br></pre></td></tr></table></figure>

<p>&#x8FD0;&#x884C;&#x793A;&#x4F8B;&#x9996;&#x5148;&#x6253;&#x5370;&#x51FA;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x7684;&#x6570;&#x91CF;&#xFF08;8092&#xFF09;&#x548C;&#x5E72;&#x51C0;&#x8BCD;&#x6C47;&#x91CF;&#x7684;&#x89C4;&#x6A21;&#xFF08;8763 &#x4E2A;&#x5355;&#x8BCD;&#xFF09;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;Loaded: 8,092</span><br><span class="line">Vocabulary Size: 8,763</span><br></pre></td></tr></table></figure>

<p>&#x6700;&#x540E;&#xFF0C;&#x628A;&#x5E72;&#x51C0;&#x7684;&#x63CF;&#x8FF0;&#x5199;&#x5165; descriptions.txt&#x3002;</p>
<p>&#x67E5;&#x770B;&#x6587;&#x4EF6;&#xFF0C;&#x6211;&#x4EEC;&#x80FD;&#x591F;&#x770B;&#x5230;&#x8BE5;&#x63CF;&#x8FF0;&#x53EF;&#x7528;&#x4E8E;&#x5EFA;&#x6A21;&#x3002;&#x6587;&#x4EF6;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x987A;&#x5E8F;&#x53EF;&#x80FD;&#x4F1A;&#x53D1;&#x751F;&#x6539;&#x53D8;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;2252123185_487f21e336 bunch on people are seated in stadium</span><br><span class="line">2252123185_487f21e336 crowded stadium is full of people watching an event</span><br><span class="line">2252123185_487f21e336 crowd of people fill up packed stadium</span><br><span class="line">2252123185_487f21e336 crowd sitting in an indoor stadium</span><br><span class="line">2252123185_487f21e336 stadium full of people watch game</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><strong>&#x5F00;&#x53D1;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;</strong></p>
<p>&#x672C;&#x8282;&#x6211;&#x4EEC;&#x5C06;&#x5B9A;&#x4E49;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#xFF0C;&#x5728;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x8FDB;&#x884C;&#x62DF;&#x5408;&#x3002;&#x672C;&#x8282;&#x5206;&#x4E3A;&#x4EE5;&#x4E0B;&#x51E0;&#x90E8;&#x5206;&#xFF1A;</p>
<ol>
<li>&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x3002;</li>
</ol>
<ol start="2">
<li>&#x5B9A;&#x4E49;&#x6A21;&#x578B;&#x3002;</li>
</ol>
<ol start="3">
<li>&#x62DF;&#x5408;&#x6A21;&#x578B;&#x3002;</li>
</ol>
<ol start="4">
<li>&#x5B8C;&#x6210;&#x793A;&#x4F8B;&#x3002;</li>
</ol>
<p><strong>&#x52A0;&#x8F7D;&#x6570;&#x636E;</strong></p>
<p>&#x9996;&#x5148;&#xFF0C;&#x6211;&#x4EEC;&#x5FC5;&#x987B;&#x52A0;&#x8F7D;&#x51C6;&#x5907;&#x597D;&#x7684;&#x56FE;&#x50CF;&#x548C;&#x6587;&#x672C;&#x6570;&#x636E;&#x6765;&#x62DF;&#x5408;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x5728;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x548C;&#x63CF;&#x8FF0;&#x4E0A;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x3002;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x8BA1;&#x5212;&#x5728;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x76D1;&#x63A7;&#x6A21;&#x578B;&#x6027;&#x80FD;&#xFF0C;&#x4F7F;&#x7528;&#x8BE5;&#x6027;&#x80FD;&#x786E;&#x5B9A;&#x4EC0;&#x4E48;&#x65F6;&#x5019;&#x4FDD;&#x5B58;&#x6A21;&#x578B;&#x81F3;&#x6587;&#x4EF6;&#x3002;</p>
<p>&#x8BAD;&#x7EC3;&#x548C;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x5DF2;&#x7ECF;&#x9884;&#x5236;&#x597D;&#xFF0C;&#x5E76;&#x5206;&#x522B;&#x4FDD;&#x5B58;&#x5728; Flickr_8k.trainImages.txt &#x548C; Flickr_8k.devImages.txt &#x6587;&#x4EF6;&#x4E2D;&#xFF0C;&#x4E8C;&#x8005;&#x5747;&#x5305;&#x542B;&#x56FE;&#x50CF;&#x6587;&#x4EF6;&#x540D;&#x5217;&#x8868;&#x3002;&#x4ECE;&#x8FD9;&#x4E9B;&#x6587;&#x4EF6;&#x540D;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x63D0;&#x53D6;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#xFF0C;&#x5E76;&#x4F7F;&#x7528;&#x5B83;&#x4EEC;&#x4E3A;&#x6BCF;&#x4E2A;&#x96C6;&#x8FC7;&#x6EE4;&#x56FE;&#x50CF;&#x548C;&#x63CF;&#x8FF0;&#x3002;</p>
<p>&#x5982;&#x4E0B;&#x6240;&#x793A;&#xFF0C;load_set() &#x51FD;&#x6570;&#x5C06;&#x6839;&#x636E;&#x8BAD;&#x7EC3;&#x6216;&#x5F00;&#x53D1;&#x96C6;&#x6587;&#x4EF6;&#x540D;&#x52A0;&#x8F7D;&#x4E00;&#x4E2A;&#x9884;&#x5B9A;&#x4E49;&#x6807;&#x8BC6;&#x7B26;&#x96C6;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># load a pre-defined list of photo identifiers</span><br><span class="line">def load_set(filename):</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	dataset = list()</span><br><span class="line">	# process line by line</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# skip empty lines</span><br><span class="line">		if len(line) &lt; 1:</span><br><span class="line">			continue</span><br><span class="line">		# get the image identifier</span><br><span class="line">		identifier = line.split(&apos;.&apos;)[0]</span><br><span class="line">		dataset.append(identifier)</span><br><span class="line">	return set(dataset)</span><br></pre></td></tr></table></figure>

<p>&#x73B0;&#x5728;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x9884;&#x5B9A;&#x4E49;&#x8BAD;&#x7EC3;&#x6216;&#x5F00;&#x53D1;&#x6807;&#x8BC6;&#x7B26;&#x96C6;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x548C;&#x63CF;&#x8FF0;&#x4E86;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x662F; load_clean_descriptions() &#x51FD;&#x6570;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x4ECE;&#x7ED9;&#x5B9A;&#x6807;&#x8BC6;&#x7B26;&#x96C6;&#x7684; descriptions.txt &#x4E2D;&#x52A0;&#x8F7D;&#x5E72;&#x51C0;&#x7684;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#xFF0C;&#x5E76;&#x5411;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x5217;&#x8868;&#x8FD4;&#x56DE;&#x6807;&#x8BC6;&#x7B26;&#x8BCD;&#x5178;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x8981;&#x5F00;&#x53D1;&#x7684;&#x6A21;&#x578B;&#x80FD;&#x591F;&#x751F;&#x6210;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x7684;&#x5B57;&#x5E55;&#xFF0C;&#x4E00;&#x6B21;&#x751F;&#x6210;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#x3002;&#x5148;&#x524D;&#x751F;&#x6210;&#x7684;&#x5355;&#x8BCD;&#x5E8F;&#x5217;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x4E00;&#x4E2A; first word &#x6765;&#x5F00;&#x542F;&#x751F;&#x6210;&#x6B65;&#x9AA4;&#x548C;&#x4E00;&#x4E2A; last word &#x6765;&#x8868;&#x793A;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x7ED3;&#x675F;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528;&#x5B57;&#x7B26;&#x4E32; startseq &#x548C; endseq &#x5B8C;&#x6210;&#x8BE5;&#x76EE;&#x7684;&#x3002;&#x8FD9;&#x4E9B;&#x6807;&#x8BB0;&#x88AB;&#x6DFB;&#x52A0;&#x81F3;&#x52A0;&#x8F7D;&#x63CF;&#x8FF0;&#xFF0C;&#x50CF;&#x5B83;&#x4EEC;&#x672C;&#x8EAB;&#x5C31;&#x662F;&#x52A0;&#x8F7D;&#x51FA;&#x7684;&#x90A3;&#x6837;&#x3002;&#x5728;&#x5BF9;&#x6587;&#x672C;&#x8FDB;&#x884C;&#x7F16;&#x7801;&#x4E4B;&#x524D;&#x8FDB;&#x884C;&#x8BE5;&#x64CD;&#x4F5C;&#x975E;&#x5E38;&#x91CD;&#x8981;&#xFF0C;&#x8FD9;&#x6837;&#x8FD9;&#x4E9B;&#x6807;&#x8BB0;&#x624D;&#x80FD;&#x5F97;&#x5230;&#x6B63;&#x786E;&#x7F16;&#x7801;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load clean descriptions into memory</span><br><span class="line">def load_clean_descriptions(filename, dataset):</span><br><span class="line">	# load document</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	descriptions = dict()</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		# split id from description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# skip images not in the set</span><br><span class="line">		if image_id in dataset:</span><br><span class="line">			# create list</span><br><span class="line">			if image_id not in descriptions:</span><br><span class="line">				descriptions[image_id] = list()</span><br><span class="line">			# wrap description in tokens</span><br><span class="line">			desc = &apos;startseq &apos; + &apos; &apos;.join(image_desc) + &apos; endseq&apos;</span><br><span class="line">			# store</span><br><span class="line">			descriptions[image_id].append(desc)</span><br><span class="line">	return descriptions</span><br></pre></td></tr></table></figure>

<p>&#x63A5;&#x4E0B;&#x6765;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4E3A;&#x7ED9;&#x5B9A;&#x6570;&#x636E;&#x96C6;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x5B9A;&#x4E49;&#x4E86; load_photo_features() &#x51FD;&#x6570;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x52A0;&#x8F7D;&#x4E86;&#x6574;&#x4E2A;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x96C6;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x96C6;&#x4F60;&#x611F;&#x5174;&#x8DA3;&#x7684;&#x5B50;&#x96C6;&#x3002;</p>
<p>&#x8FD9;&#x4E0D;&#x662F;&#x5F88;&#x9AD8;&#x6548;&#xFF0C;&#x4F46;&#x662F;&#xFF0C;&#x8FD9;&#x53EF;&#x4EE5;&#x5E2E;&#x52A9;&#x6211;&#x4EEC;&#x542F;&#x52A8;&#xFF0C;&#x5FEB;&#x901F;&#x8FD0;&#x884C;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load photo features</span><br><span class="line">def load_photo_features(filename, dataset):</span><br><span class="line">	# load all features</span><br><span class="line">	all_features = load(open(filename, &apos;rb&apos;))</span><br><span class="line">	# filter features</span><br><span class="line">	features = {k: all_features[k] for k in dataset}</span><br><span class="line">	return features</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5728;&#x8FD9;&#x91CC;&#x6682;&#x505C;&#x4E00;&#x4E0B;&#xFF0C;&#x6D4B;&#x8BD5;&#x76EE;&#x524D;&#x5F00;&#x53D1;&#x7684;&#x6240;&#x6709;&#x5185;&#x5BB9;&#x3002;</p>
<p>&#x5B8C;&#x6574;&#x7684;&#x4EE3;&#x7801;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># load a pre-defined list of photo identifiers</span><br><span class="line">def load_set(filename):</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	dataset = list()</span><br><span class="line">	# process line by line</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# skip empty lines</span><br><span class="line">		if len(line) &lt; 1:</span><br><span class="line">			continue</span><br><span class="line">		# get the image identifier</span><br><span class="line">		identifier = line.split(&apos;.&apos;)[0]</span><br><span class="line">		dataset.append(identifier)</span><br><span class="line">	return set(dataset)</span><br><span class="line"></span><br><span class="line"># load clean descriptions into memory</span><br><span class="line">def load_clean_descriptions(filename, dataset):</span><br><span class="line">	# load document</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	descriptions = dict()</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		# split id from description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# skip images not in the set</span><br><span class="line">		if image_id in dataset:</span><br><span class="line">			# create list</span><br><span class="line">			if image_id not in descriptions:</span><br><span class="line">				descriptions[image_id] = list()</span><br><span class="line">			# wrap description in tokens</span><br><span class="line">			desc = &apos;startseq &apos; + &apos; &apos;.join(image_desc) + &apos; endseq&apos;</span><br><span class="line">			# store</span><br><span class="line">			descriptions[image_id].append(desc)</span><br><span class="line">	return descriptions</span><br><span class="line"></span><br><span class="line"># load photo features</span><br><span class="line">def load_photo_features(filename, dataset):</span><br><span class="line">	# load all features</span><br><span class="line">	all_features = load(open(filename, &apos;rb&apos;))</span><br><span class="line">	# filter features</span><br><span class="line">	features = {k: all_features[k] for k in dataset}</span><br><span class="line">	return features</span><br><span class="line"></span><br><span class="line"># load training dataset (6K)</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.trainImages.txt&apos;</span><br><span class="line">train = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(train))</span><br><span class="line"># descriptions</span><br><span class="line">train_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, train)</span><br><span class="line">print(&apos;Descriptions: train=%d&apos; % len(train_descriptions))</span><br><span class="line"># photo features</span><br><span class="line">train_features = load_photo_features(&apos;features.pkl&apos;, train)</span><br><span class="line">print(&apos;Photos: train=%d&apos; % len(train_features))</span><br></pre></td></tr></table></figure>

<p>&#x8FD0;&#x884C;&#x8BE5;&#x793A;&#x4F8B;&#x9996;&#x5148;&#x5728;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x52A0;&#x8F7D; 6000 &#x5F20;&#x56FE;&#x50CF;&#x6807;&#x8BC6;&#x7B26;&#x3002;&#x8FD9;&#x4E9B;&#x7279;&#x5F81;&#x4E4B;&#x540E;&#x5C06;&#x7528;&#x4E8E;&#x52A0;&#x8F7D;&#x5E72;&#x51C0;&#x63CF;&#x8FF0;&#x6587;&#x672C;&#x548C;&#x9884;&#x8BA1;&#x7B97;&#x7684;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;Dataset: 6,000</span><br><span class="line">Descriptions: train=6,000</span><br><span class="line">Photos: train=6,000</span><br></pre></td></tr></table></figure>

<p>&#x63CF;&#x8FF0;&#x6587;&#x672C;&#x5728;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x9988;&#x9001;&#x81F3;&#x6A21;&#x578B;&#x6216;&#x4E0E;&#x6A21;&#x578B;&#x9884;&#x6D4B;&#x8FDB;&#x884C;&#x5BF9;&#x6BD4;&#x4E4B;&#x524D;&#x9700;&#x8981;&#x5148;&#x7F16;&#x7801;&#x6210;&#x6570;&#x503C;&#x3002;</p>
<p>&#x7F16;&#x7801;&#x6570;&#x636E;&#x7684;&#x7B2C;&#x4E00;&#x6B65;&#x662F;&#x521B;&#x5EFA;&#x5355;&#x8BCD;&#x5230;&#x552F;&#x4E00;&#x6574;&#x6570;&#x503C;&#x4E4B;&#x95F4;&#x7684;&#x6301;&#x7EED;&#x6620;&#x5C04;&#x3002;Keras &#x63D0;&#x4F9B; Tokenizer class&#xFF0C;&#x53EF;&#x6839;&#x636E;&#x52A0;&#x8F7D;&#x7684;&#x63CF;&#x8FF0;&#x6570;&#x636E;&#x5B66;&#x4E60;&#x8BE5;&#x6620;&#x5C04;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x5B9A;&#x4E49;&#x4E86;&#x7528;&#x4E8E;&#x5C06;&#x63CF;&#x8FF0;&#x8BCD;&#x5178;&#x8F6C;&#x6362;&#x6210;&#x5B57;&#x7B26;&#x4E32;&#x5217;&#x8868;&#x7684; to_lines() &#x51FD;&#x6570;&#xFF0C;&#x548C;&#x5BF9;&#x52A0;&#x8F7D;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6587;&#x672C;&#x62DF;&#x5408; Tokenizer &#x7684; create_tokenizer() &#x51FD;&#x6570;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# convert a dictionary of clean descriptions to a list of descriptions</span><br><span class="line">def to_lines(descriptions):</span><br><span class="line">	all_desc = list()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.append(d) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># fit a tokenizer given caption descriptions</span><br><span class="line">def create_tokenizer(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	tokenizer = Tokenizer()</span><br><span class="line">	tokenizer.fit_on_texts(lines)</span><br><span class="line">	return tokenizer</span><br><span class="line"></span><br><span class="line"># prepare tokenizer</span><br><span class="line">tokenizer = create_tokenizer(train_descriptions)</span><br><span class="line">vocab_size = len(tokenizer.word_index) + 1</span><br><span class="line">print(&apos;Vocabulary Size: %d&apos; % vocab_size)</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x73B0;&#x5728;&#x5BF9;&#x6587;&#x672C;&#x8FDB;&#x884C;&#x7F16;&#x7801;&#x3002;</p>
<p>&#x6BCF;&#x4E2A;&#x63CF;&#x8FF0;&#x5C06;&#x88AB;&#x5206;&#x5272;&#x6210;&#x5355;&#x8BCD;&#x3002;&#x6211;&#x4EEC;&#x5411;&#x8BE5;&#x6A21;&#x578B;&#x63D0;&#x4F9B;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#x548C;&#x56FE;&#x50CF;&#xFF0C;&#x7136;&#x540E;&#x6A21;&#x578B;&#x751F;&#x6210;&#x4E0B;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#x3002;&#x63CF;&#x8FF0;&#x7684;&#x524D;&#x4E24;&#x4E2A;&#x5355;&#x8BCD;&#x548C;&#x56FE;&#x50CF;&#x5C06;&#x4F5C;&#x4E3A;&#x6A21;&#x578B;&#x8F93;&#x5165;&#x4EE5;&#x751F;&#x6210;&#x4E0B;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#xFF0C;&#x8FD9;&#x5C31;&#x662F;&#x8BE5;&#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#x65B9;&#x5F0F;&#x3002;</p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x300C;a little girl running in field&#x300D;&#x5C06;&#x88AB;&#x5206;&#x5272;&#x6210; 6 &#x4E2A;&#x8F93;&#x5165;-&#x8F93;&#x51FA;&#x5BF9;&#x6765;&#x8BAD;&#x7EC3;&#x8BE5;&#x6A21;&#x578B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;X1,		X2 (text sequence), 						y (word)</span><br><span class="line">photo	startseq, 									little</span><br><span class="line">photo	startseq, little,							girl</span><br><span class="line">photo	startseq, little, girl, 					running</span><br><span class="line">photo	startseq, little, girl, running, 			in</span><br><span class="line">photo	startseq, little, girl, running, in, 		field</span><br><span class="line">photo	startseq, little, girl, running, in, field, endseq</span><br></pre></td></tr></table></figure>

<p>&#x7A0D;&#x540E;&#xFF0C;&#x5F53;&#x6A21;&#x578B;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#x65F6;&#xFF0C;&#x751F;&#x6210;&#x7684;&#x5355;&#x8BCD;&#x5C06;&#x88AB;&#x8FDE;&#x7ED3;&#x8D77;&#x6765;&#xFF0C;&#x9012;&#x5F52;&#x5730;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x4EE5;&#x751F;&#x6210;&#x56FE;&#x50CF;&#x5B57;&#x5E55;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x662F; create_sequences() &#x51FD;&#x6570;&#xFF0C;&#x7ED9;&#x51FA; tokenizer&#x3001;&#x6700;&#x5927;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x548C;&#x6240;&#x6709;&#x63CF;&#x8FF0;&#x548C;&#x56FE;&#x50CF;&#x7684;&#x8BCD;&#x5178;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x8FD9;&#x4E9B;&#x6570;&#x636E;&#x8F6C;&#x6362;&#x6210;&#x8F93;&#x5165;-&#x8F93;&#x51FA;&#x5BF9;&#x6765;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x3002;&#x8BE5;&#x6A21;&#x578B;&#x6709;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x6570;&#x7EC4;&#xFF1A;&#x4E00;&#x4E2A;&#x7528;&#x4E8E;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#xFF0C;&#x4E00;&#x4E2A;&#x7528;&#x4E8E;&#x7F16;&#x7801;&#x6587;&#x672C;&#x3002;&#x6A21;&#x578B;&#x8F93;&#x51FA;&#x662F;&#x6587;&#x672C;&#x5E8F;&#x5217;&#x4E2D;&#x7F16;&#x7801;&#x7684;&#x4E0B;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#x3002;</p>
<p>&#x8F93;&#x5165;&#x6587;&#x672C;&#x88AB;&#x7F16;&#x7801;&#x4E3A;&#x6574;&#x6570;&#xFF0C;&#x88AB;&#x9988;&#x9001;&#x81F3;&#x8BCD;&#x5D4C;&#x5165;&#x5C42;&#x3002;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x5C06;&#x88AB;&#x76F4;&#x63A5;&#x9988;&#x9001;&#x81F3;&#x6A21;&#x578B;&#x7684;&#x53E6;&#x4E00;&#x90E8;&#x5206;&#x3002;&#x8BE5;&#x6A21;&#x578B;&#x8F93;&#x51FA;&#x7684;&#x9884;&#x6D4B;&#x662F;&#x6240;&#x6709;&#x5355;&#x8BCD;&#x5728;&#x8BCD;&#x6C47;&#x8868;&#x4E2D;&#x7684;&#x6982;&#x7387;&#x5206;&#x5E03;&#x3002;</p>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x8F93;&#x51FA;&#x6570;&#x636E;&#x662F;&#x6BCF;&#x4E2A;&#x5355;&#x8BCD;&#x7684; one-hot &#x7F16;&#x7801;&#xFF0C;&#x5B83;&#x8868;&#x793A;&#x4E00;&#x79CD;&#x7406;&#x60F3;&#x5316;&#x7684;&#x6982;&#x7387;&#x5206;&#x5E03;&#xFF0C;&#x5373;&#x9664;&#x4E86;&#x5B9E;&#x9645;&#x8BCD;&#x4F4D;&#x7F6E;&#x4E4B;&#x5916;&#x6240;&#x6709;&#x8BCD;&#x4F4D;&#x7F6E;&#x7684;&#x503C;&#x90FD;&#x4E3A; 0&#xFF0C;&#x5B9E;&#x9645;&#x8BCD;&#x4F4D;&#x7F6E;&#x7684;&#x503C;&#x4E3A; 1&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# create sequences of images, input sequences and output words for an image</span><br><span class="line">def create_sequences(tokenizer, max_length, descriptions, photos):</span><br><span class="line">	X1, X2, y = list(), list(), list()</span><br><span class="line">	# walk through each image identifier</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		# walk through each description for the image</span><br><span class="line">		for desc in desc_list:</span><br><span class="line">			# encode the sequence</span><br><span class="line">			seq = tokenizer.texts_to_sequences([desc])[0]</span><br><span class="line">			# split one sequence into multiple X,y pairs</span><br><span class="line">			for i in range(1, len(seq)):</span><br><span class="line">				# split into input and output pair</span><br><span class="line">				in_seq, out_seq = seq[:i], seq[i]</span><br><span class="line">				# pad input sequence</span><br><span class="line">				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]</span><br><span class="line">				# encode output sequence</span><br><span class="line">				out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]</span><br><span class="line">				# store</span><br><span class="line">				X1.append(photos[key][0])</span><br><span class="line">				X2.append(in_seq)</span><br><span class="line">				y.append(out_seq)</span><br><span class="line">	return array(X1), array(X2), array(y)</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x9700;&#x8981;&#x8BA1;&#x7B97;&#x6700;&#x957F;&#x63CF;&#x8FF0;&#x4E2D;&#x5355;&#x8BCD;&#x7684;&#x6700;&#x5927;&#x6570;&#x91CF;&#x3002;&#x4E0B;&#x9762;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x5E2E;&#x52A9;&#x7684;&#x51FD;&#x6570; max_length()&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# calculate the length of the description with the most words</span><br><span class="line">def max_length(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	return max(len(d.split()) for d in lines)</span><br></pre></td></tr></table></figure>

<p>&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4E3A;&#x8BAD;&#x7EC3;&#x548C;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x52A0;&#x8F7D;&#x6570;&#x636E;&#xFF0C;&#x5E76;&#x5C06;&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x8F6C;&#x6362;&#x6210;&#x8F93;&#x5165;-&#x8F93;&#x51FA;&#x5BF9;&#x6765;&#x62DF;&#x5408;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#x3002;</p>
<p><strong>&#x5B9A;&#x4E49;&#x6A21;&#x578B;</strong></p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x6839;&#x636E; Marc Tanti, et al. &#x5728; 2017 &#x5E74;&#x8BBA;&#x6587;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x300C;merge-model&#x300D;&#x5B9A;&#x4E49;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#x3002;</p>
<ul>
<li>Where to put the Image in an Image Caption Generator&#xFF0C;2017</li>
<li>What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?&#xFF0C;2017</li>
</ul>
<p>&#x8BBA;&#x6587;&#x4F5C;&#x8005;&#x63D0;&#x4F9B;&#x4E86;&#x8BE5;&#x6A21;&#x578B;&#x7684;&#x7B80;&#x56FE;&#xFF0C;&#x5982;&#x4E0B;&#x6240;&#x793A;&#xFF1A;</p>
<p><img src="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/937c3700b1e6a62adc72e1ce3f87c1a34786fdd6a5fc1ae559cf9a206ba7483a" alt>  </p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x4ECE;&#x4E09;&#x90E8;&#x5206;&#x63CF;&#x8FF0;&#x8BE5;&#x6A21;&#x578B;&#xFF1A;</p>
<ul>
<li>&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x63D0;&#x53D6;&#x5668;&#xFF1A;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x5728; ImageNet &#x6570;&#x636E;&#x96C6;&#x4E0A;&#x9884;&#x8BAD;&#x7EC3;&#x7684; 16 &#x5C42; VGG &#x6A21;&#x578B;&#x3002;&#x6211;&#x4EEC;&#x5DF2;&#x7ECF;&#x4F7F;&#x7528; VGG &#x6A21;&#x578B;&#xFF08;&#x6CA1;&#x6709;&#x8F93;&#x51FA;&#x5C42;&#xFF09;&#x5BF9;&#x56FE;&#x50CF;&#x8FDB;&#x884C;&#x9884;&#x5904;&#x7406;&#xFF0C;&#x5E76;&#x5C06;&#x4F7F;&#x7528;&#x8BE5;&#x6A21;&#x578B;&#x9884;&#x6D4B;&#x7684;&#x63D0;&#x53D6;&#x7279;&#x5F81;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x3002;</li>
<li>&#x5E8F;&#x5217;&#x5904;&#x7406;&#x5668;&#xFF1A;&#x5408;&#x9002;&#x4E00;&#x4E2A;&#x8BCD;&#x5D4C;&#x5165;&#x5C42;&#xFF0C;&#x7528;&#x4E8E;&#x5904;&#x7406;&#x6587;&#x672C;&#x8F93;&#x5165;&#xFF0C;&#x540E;&#x9762;&#x662F;&#x957F;&#x77ED;&#x671F;&#x8BB0;&#x5FC6;&#xFF08;LSTM&#xFF09;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5C42;&#x3002;</li>
<li>&#x89E3;&#x7801;&#x5668;&#xFF1A;&#x7279;&#x5F81;&#x63D0;&#x53D6;&#x5668;&#x548C;&#x5E8F;&#x5217;&#x5904;&#x7406;&#x5668;&#x8F93;&#x51FA;&#x4E00;&#x4E2A;&#x56FA;&#x5B9A;&#x957F;&#x5EA6;&#x5411;&#x91CF;&#x3002;&#x8FD9;&#x4E9B;&#x5411;&#x91CF;&#x7531;&#x5BC6;&#x96C6;&#x5C42;&#xFF08;Dense layer&#xFF09;&#x878D;&#x5408;&#x548C;&#x5904;&#x7406;&#xFF0C;&#x6765;&#x8FDB;&#x884C;&#x6700;&#x7EC8;&#x9884;&#x6D4B;&#x3002;</li>
</ul>
<p>&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x63D0;&#x53D6;&#x5668;&#x6A21;&#x578B;&#x7684;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x662F;&#x7EF4;&#x5EA6;&#x4E3A; 4096 &#x7684;&#x5411;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x5411;&#x91CF;&#x7ECF;&#x8FC7;&#x5168;&#x8FDE;&#x63A5;&#x5C42;&#x5904;&#x7406;&#x5E76;&#x751F;&#x6210;&#x56FE;&#x50CF;&#x7684; 256 &#x5143;&#x7D20;&#x8868;&#x5F81;&#x3002;</p>
<p>&#x5E8F;&#x5217;&#x5904;&#x7406;&#x5668;&#x6A21;&#x578B;&#x671F;&#x671B;&#x9988;&#x9001;&#x81F3;&#x5D4C;&#x5165;&#x5C42;&#x7684;&#x9884;&#x5B9A;&#x4E49;&#x957F;&#x5EA6;&#xFF08;34 &#x4E2A;&#x5355;&#x8BCD;&#xFF09;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x4F7F;&#x7528;&#x63A9;&#x7801;&#x6765;&#x5FFD;&#x7565; padded &#x503C;&#x3002;&#x4E4B;&#x540E;&#x662F;&#x5177;&#x5907; 256 &#x4E2A;&#x5FAA;&#x73AF;&#x5355;&#x5143;&#x7684; LSTM &#x5C42;&#x3002;</p>
<p>&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x6A21;&#x578B;&#x5747;&#x8F93;&#x51FA; 256 &#x5143;&#x7D20;&#x7684;&#x5411;&#x91CF;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x8F93;&#x5165;&#x6A21;&#x578B;&#x4EE5; 50% &#x7684; dropout &#x7387;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#xFF0C;&#x65E8;&#x5728;&#x51CF;&#x5C11;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x7684;&#x8FC7;&#x62DF;&#x5408;&#x60C5;&#x51B5;&#xFF0C;&#x56E0;&#x4E3A;&#x8BE5;&#x6A21;&#x578B;&#x914D;&#x7F6E;&#x5B66;&#x4E60;&#x975E;&#x5E38;&#x5FEB;&#x3002;</p>
<p>&#x89E3;&#x7801;&#x5668;&#x6A21;&#x578B;&#x4F7F;&#x7528;&#x989D;&#x5916;&#x7684;&#x64CD;&#x4F5C;&#x878D;&#x5408;&#x6765;&#x81EA;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x6A21;&#x578B;&#x7684;&#x5411;&#x91CF;&#x3002;&#x7136;&#x540E;&#x5C06;&#x5176;&#x9988;&#x9001;&#x81F3; 256 &#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x7684;&#x5BC6;&#x96C6;&#x5C42;&#xFF0C;&#x7136;&#x540E;&#x8F93;&#x9001;&#x81F3;&#x6700;&#x7EC8;&#x8F93;&#x51FA;&#x5BC6;&#x96C6;&#x5C42;&#xFF0C;&#x4ECE;&#x800C;&#x5728;&#x6240;&#x6709;&#x8F93;&#x51FA;&#x8BCD;&#x6C47;&#x4E0A;&#x5BF9;&#x5E8F;&#x5217;&#x4E2D;&#x7684;&#x4E0B;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#x8FDB;&#x884C; softmax &#x9884;&#x6D4B;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x7684; define_model() &#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x548C;&#x8FD4;&#x56DE;&#x8981;&#x62DF;&#x5408;&#x7684;&#x6A21;&#x578B;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# define the captioning model</span><br><span class="line">def define_model(vocab_size, max_length):</span><br><span class="line">	# feature extractor model</span><br><span class="line">	inputs1 = Input(shape=(4096,))</span><br><span class="line">	fe1 = Dropout(0.5)(inputs1)</span><br><span class="line">	fe2 = Dense(256, activation=&apos;relu&apos;)(fe1)</span><br><span class="line">	# sequence model</span><br><span class="line">	inputs2 = Input(shape=(max_length,))</span><br><span class="line">	se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)</span><br><span class="line">	se2 = Dropout(0.5)(se1)</span><br><span class="line">	se3 = LSTM(256)(se2)</span><br><span class="line">	# decoder model</span><br><span class="line">	decoder1 = add([fe2, se3])</span><br><span class="line">	decoder2 = Dense(256, activation=&apos;relu&apos;)(decoder1)</span><br><span class="line">	outputs = Dense(vocab_size, activation=&apos;softmax&apos;)(decoder2)</span><br><span class="line">	# tie it together [image, seq] [word]</span><br><span class="line">	model = Model(inputs=[inputs1, inputs2], outputs=outputs)</span><br><span class="line">	model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;adam&apos;)</span><br><span class="line">	# summarize model</span><br><span class="line">	print(model.summary())</span><br><span class="line">	plot_model(model, to_file=&apos;model.png&apos;, show_shapes=True)</span><br><span class="line">	return model</span><br></pre></td></tr></table></figure>

<p>&#x8981;&#x4E86;&#x89E3;&#x6A21;&#x578B;&#x7ED3;&#x6784;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x5C42;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x8BF7;&#x53C2;&#x8003;&#x4E0B;&#x8868;&#x4E2D;&#x7684;&#x603B;&#x7ED3;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;____________________________________________________________________________________________________</span><br><span class="line">Layer (type)                     Output Shape          Param #     Connected to</span><br><span class="line">====================================================================================================</span><br><span class="line">input_2 (InputLayer)             (None, 34)            0</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">input_1 (InputLayer)             (None, 4096)          0</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">embedding_1 (Embedding)          (None, 34, 256)       1940224     input_2[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)              (None, 4096)          0           input_1[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)              (None, 34, 256)       0           embedding_1[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dense_1 (Dense)                  (None, 256)           1048832     dropout_1[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">lstm_1 (LSTM)                    (None, 256)           525312      dropout_2[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">add_1 (Add)                      (None, 256)           0           dense_1[0][0]</span><br><span class="line">                                                                   lstm_1[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dense_2 (Dense)                  (None, 256)           65792       add_1[0][0]</span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dense_3 (Dense)                  (None, 7579)          1947803     dense_2[0][0]</span><br><span class="line">====================================================================================================</span><br><span class="line">Total params: 5,527,963</span><br><span class="line">Trainable params: 5,527,963</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">____________________________________________________________________________________________________</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x8FD8;&#x521B;&#x5EFA;&#x4E86;&#x4E00;&#x5E45;&#x56FE;&#x6765;&#x53EF;&#x89C6;&#x5316;&#x7F51;&#x7EDC;&#x7ED3;&#x6784;&#xFF0C;&#x5E2E;&#x52A9;&#x7406;&#x89E3;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x6D41;&#x3002;</p>
<p><img src="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/073fa794e019060436dfb976fb5e1c2c73469fee0071901f1d7c871acf43e4b5" alt>  </p>
<p><em>&#x56FE;&#x50CF;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#x793A;&#x610F;&#x56FE;&#x3002;</em></p>
<p><strong>&#x62DF;&#x5408;&#x6A21;&#x578B;</strong></p>
<p>&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x5DF2;&#x7ECF;&#x4E86;&#x89E3;&#x5982;&#x4F55;&#x5B9A;&#x4E49;&#x6A21;&#x578B;&#x4E86;&#xFF0C;&#x90A3;&#x4E48;&#x63A5;&#x4E0B;&#x6765;&#x6211;&#x4EEC;&#x8981;&#x5728;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x62DF;&#x5408;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x578B;&#x5B66;&#x4E60;&#x901F;&#x5EA6;&#x5FEB;&#xFF0C;&#x5F88;&#x5FEB;&#x5C31;&#x4F1A;&#x5BF9;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4EA7;&#x751F;&#x8FC7;&#x62DF;&#x5408;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x5728;&#x7559;&#x51FA;&#x7684;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x76D1;&#x63A7;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x7684;&#x6CDB;&#x5316;&#x60C5;&#x51B5;&#x3002;&#x5982;&#x679C;&#x6A21;&#x578B;&#x5728;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x7684;&#x6280;&#x80FD;&#x5728;&#x6BCF;&#x4E2A; epoch &#x7ED3;&#x675F;&#x65F6;&#x6709;&#x6240;&#x63D0;&#x5347;&#xFF0C;&#x5219;&#x6211;&#x4EEC;&#x5C06;&#x6574;&#x4E2A;&#x6A21;&#x578B;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#x3002;</p>
<p>&#x5728;&#x8FD0;&#x884C;&#x7ED3;&#x675F;&#x65F6;&#xFF0C;&#x6211;&#x4EEC;&#x80FD;&#x591F;&#x4F7F;&#x7528;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x5177;&#x5907;&#x6700;&#x4F18;&#x6280;&#x80FD;&#x7684;&#x6A21;&#x578B;&#x4F5C;&#x4E3A;&#x6700;&#x7EC8;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x901A;&#x8FC7;&#x5728; Keras &#x4E2D;&#x5B9A;&#x4E49; ModelCheckpoint&#xFF0C;&#x4F7F;&#x4E4B;&#x76D1;&#x63A7;&#x9A8C;&#x8BC1;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x7684;&#x6700;&#x5C0F;&#x635F;&#x5931;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5B9E;&#x73B0;&#x4EE5;&#x4E0A;&#x76EE;&#x7684;&#x3002;&#x7136;&#x540E;&#x5C06;&#x8BE5;&#x6A21;&#x578B;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#x540D;&#x4E2D;&#x5305;&#x542B;&#x8BAD;&#x7EC3;&#x635F;&#x5931;&#x548C;&#x9A8C;&#x8BC1;&#x635F;&#x5931;&#x7684;&#x6587;&#x4EF6;&#x4E2D;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# define checkpoint callback</span><br><span class="line">filepath = &apos;model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5&apos;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&apos;val_loss&apos;, verbose=1, save_best_only=True, mode=&apos;min&apos;)</span><br></pre></td></tr></table></figure>

<p>&#x4E4B;&#x540E;&#xFF0C;&#x901A;&#x8FC7; fit() &#x4E2D;&#x7684; callbacks &#x53C2;&#x6570;&#x6307;&#x5B9A;&#x68C0;&#x67E5;&#x70B9;&#x3002;&#x6211;&#x4EEC;&#x8FD8;&#x9700;&#x8981; fit() &#x4E2D;&#x7684; validation_data &#x53C2;&#x6570;&#x6307;&#x5B9A;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x4EC5;&#x62DF;&#x5408;&#x6A21;&#x578B; 20 epoch&#xFF0C;&#x7ED9;&#x51FA;&#x4E00;&#x5B9A;&#x91CF;&#x7684;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#xFF0C;&#x5728;&#x4E00;&#x822C;&#x786C;&#x4EF6;&#x4E0A;&#x6BCF;&#x4E2A; epoch &#x53EF;&#x80FD;&#x9700;&#x8981; 30 &#x5206;&#x949F;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# fit model</span><br><span class="line">model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))</span><br></pre></td></tr></table></figure>

<p><strong>&#x5B8C;&#x6210;&#x793A;&#x4F8B;</strong></p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x4E0A;&#x62DF;&#x5408;&#x6A21;&#x578B;&#x7684;&#x5B8C;&#x6574;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;from numpy import array</span><br><span class="line">from pickle import load</span><br><span class="line">from keras.preprocessing.text import Tokenizer</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from keras.utils import to_categorical</span><br><span class="line">from keras.utils import plot_model</span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.layers import Input</span><br><span class="line">from keras.layers import Dense</span><br><span class="line">from keras.layers import LSTM</span><br><span class="line">from keras.layers import Embedding</span><br><span class="line">from keras.layers import Dropout</span><br><span class="line">from keras.layers.merge import add</span><br><span class="line">from keras.callbacks import ModelCheckpoint</span><br><span class="line"></span><br><span class="line"># load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># load a pre-defined list of photo identifiers</span><br><span class="line">def load_set(filename):</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	dataset = list()</span><br><span class="line">	# process line by line</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# skip empty lines</span><br><span class="line">		if len(line) &lt; 1:</span><br><span class="line">			continue</span><br><span class="line">		# get the image identifier</span><br><span class="line">		identifier = line.split(&apos;.&apos;)[0]</span><br><span class="line">		dataset.append(identifier)</span><br><span class="line">	return set(dataset)</span><br><span class="line"></span><br><span class="line"># load clean descriptions into memory</span><br><span class="line">def load_clean_descriptions(filename, dataset):</span><br><span class="line">	# load document</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	descriptions = dict()</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		# split id from description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# skip images not in the set</span><br><span class="line">		if image_id in dataset:</span><br><span class="line">			# create list</span><br><span class="line">			if image_id not in descriptions:</span><br><span class="line">				descriptions[image_id] = list()</span><br><span class="line">			# wrap description in tokens</span><br><span class="line">			desc = &apos;startseq &apos; + &apos; &apos;.join(image_desc) + &apos; endseq&apos;</span><br><span class="line">			# store</span><br><span class="line">			descriptions[image_id].append(desc)</span><br><span class="line">	return descriptions</span><br><span class="line"></span><br><span class="line"># load photo features</span><br><span class="line">def load_photo_features(filename, dataset):</span><br><span class="line">	# load all features</span><br><span class="line">	all_features = load(open(filename, &apos;rb&apos;))</span><br><span class="line">	# filter features</span><br><span class="line">	features = {k: all_features[k] for k in dataset}</span><br><span class="line">	return features</span><br><span class="line"></span><br><span class="line"># covert a dictionary of clean descriptions to a list of descriptions</span><br><span class="line">def to_lines(descriptions):</span><br><span class="line">	all_desc = list()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.append(d) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># fit a tokenizer given caption descriptions</span><br><span class="line">def create_tokenizer(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	tokenizer = Tokenizer()</span><br><span class="line">	tokenizer.fit_on_texts(lines)</span><br><span class="line">	return tokenizer</span><br><span class="line"></span><br><span class="line"># calculate the length of the description with the most words</span><br><span class="line">def max_length(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	return max(len(d.split()) for d in lines)</span><br><span class="line"></span><br><span class="line"># create sequences of images, input sequences and output words for an image</span><br><span class="line">def create_sequences(tokenizer, max_length, descriptions, photos):</span><br><span class="line">	X1, X2, y = list(), list(), list()</span><br><span class="line">	# walk through each image identifier</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		# walk through each description for the image</span><br><span class="line">		for desc in desc_list:</span><br><span class="line">			# encode the sequence</span><br><span class="line">			seq = tokenizer.texts_to_sequences([desc])[0]</span><br><span class="line">			# split one sequence into multiple X,y pairs</span><br><span class="line">			for i in range(1, len(seq)):</span><br><span class="line">				# split into input and output pair</span><br><span class="line">				in_seq, out_seq = seq[:i], seq[i]</span><br><span class="line">				# pad input sequence</span><br><span class="line">				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]</span><br><span class="line">				# encode output sequence</span><br><span class="line">				out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]</span><br><span class="line">				# store</span><br><span class="line">				X1.append(photos[key][0])</span><br><span class="line">				X2.append(in_seq)</span><br><span class="line">				y.append(out_seq)</span><br><span class="line">	return array(X1), array(X2), array(y)</span><br><span class="line"></span><br><span class="line"># define the captioning model</span><br><span class="line">def define_model(vocab_size, max_length):</span><br><span class="line">	# feature extractor model</span><br><span class="line">	inputs1 = Input(shape=(4096,))</span><br><span class="line">	fe1 = Dropout(0.5)(inputs1)</span><br><span class="line">	fe2 = Dense(256, activation=&apos;relu&apos;)(fe1)</span><br><span class="line">	# sequence model</span><br><span class="line">	inputs2 = Input(shape=(max_length,))</span><br><span class="line">	se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)</span><br><span class="line">	se2 = Dropout(0.5)(se1)</span><br><span class="line">	se3 = LSTM(256)(se2)</span><br><span class="line">	# decoder model</span><br><span class="line">	decoder1 = add([fe2, se3])</span><br><span class="line">	decoder2 = Dense(256, activation=&apos;relu&apos;)(decoder1)</span><br><span class="line">	outputs = Dense(vocab_size, activation=&apos;softmax&apos;)(decoder2)</span><br><span class="line">	# tie it together [image, seq] [word]</span><br><span class="line">	model = Model(inputs=[inputs1, inputs2], outputs=outputs)</span><br><span class="line">	model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;adam&apos;)</span><br><span class="line">	# summarize model</span><br><span class="line">	print(model.summary())</span><br><span class="line">	plot_model(model, to_file=&apos;model.png&apos;, show_shapes=True)</span><br><span class="line">	return model</span><br><span class="line"></span><br><span class="line"># train dataset</span><br><span class="line"></span><br><span class="line"># load training dataset (6K)</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.trainImages.txt&apos;</span><br><span class="line">train = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(train))</span><br><span class="line"># descriptions</span><br><span class="line">train_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, train)</span><br><span class="line">print(&apos;Descriptions: train=%d&apos; % len(train_descriptions))</span><br><span class="line"># photo features</span><br><span class="line">train_features = load_photo_features(&apos;features.pkl&apos;, train)</span><br><span class="line">print(&apos;Photos: train=%d&apos; % len(train_features))</span><br><span class="line"># prepare tokenizer</span><br><span class="line">tokenizer = create_tokenizer(train_descriptions)</span><br><span class="line">vocab_size = len(tokenizer.word_index) + 1</span><br><span class="line">print(&apos;Vocabulary Size: %d&apos; % vocab_size)</span><br><span class="line"># determine the maximum sequence length</span><br><span class="line">max_length = max_length(train_descriptions)</span><br><span class="line">print(&apos;Description Length: %d&apos; % max_length)</span><br><span class="line"># prepare sequences</span><br><span class="line">X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)</span><br><span class="line"></span><br><span class="line"># dev dataset</span><br><span class="line"></span><br><span class="line"># load test set</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.devImages.txt&apos;</span><br><span class="line">test = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(test))</span><br><span class="line"># descriptions</span><br><span class="line">test_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, test)</span><br><span class="line">print(&apos;Descriptions: test=%d&apos; % len(test_descriptions))</span><br><span class="line"># photo features</span><br><span class="line">test_features = load_photo_features(&apos;features.pkl&apos;, test)</span><br><span class="line">print(&apos;Photos: test=%d&apos; % len(test_features))</span><br><span class="line"># prepare sequences</span><br><span class="line">X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)</span><br><span class="line"></span><br><span class="line"># fit model</span><br><span class="line"></span><br><span class="line"># define the model</span><br><span class="line">model = define_model(vocab_size, max_length)</span><br><span class="line"># define checkpoint callback</span><br><span class="line">filepath = &apos;model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5&apos;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&apos;val_loss&apos;, verbose=1, save_best_only=True, mode=&apos;min&apos;)</span><br><span class="line"># fit model</span><br><span class="line">model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))</span><br></pre></td></tr></table></figure>

<p>&#x8FD0;&#x884C;&#x8BE5;&#x793A;&#x4F8B;&#x9996;&#x5148;&#x6253;&#x5370;&#x52A0;&#x8F7D;&#x8BAD;&#x7EC3;&#x548C;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x7684;&#x6458;&#x8981;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;Dataset: 6,000</span><br><span class="line">Descriptions: train=6,000</span><br><span class="line">Photos: train=6,000</span><br><span class="line">Vocabulary Size: 7,579</span><br><span class="line">Description Length: 34</span><br><span class="line">Dataset: 1,000</span><br><span class="line">Descriptions: test=1,000</span><br><span class="line">Photos: test=1,000</span><br></pre></td></tr></table></figure>

<p>&#x4E4B;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4E86;&#x89E3;&#x8BAD;&#x7EC3;&#x548C;&#x9A8C;&#x8BC1;&#xFF08;&#x5F00;&#x53D1;&#xFF09;&#x8F93;&#x5165;-&#x8F93;&#x51FA;&#x5BF9;&#x7684;&#x6574;&#x4F53;&#x6570;&#x91CF;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;Train on 306,404 samples, validate on 50,903 samples</span><br></pre></td></tr></table></figure>

<p>&#x7136;&#x540E;&#x8FD0;&#x884C;&#x6A21;&#x578B;&#xFF0C;&#x5C06;&#x6700;&#x4F18;&#x6A21;&#x578B;&#x4FDD;&#x5B58;&#x81F3;.h5 &#x6587;&#x4EF6;&#x3002;</p>
<p>&#x5728;&#x8FD0;&#x884C;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6211;&#x628A;&#x6700;&#x4F18;&#x9A8C;&#x8BC1;&#x7ED3;&#x679C;&#x7684;&#x6A21;&#x578B;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#x4E2D;&#xFF1A;</p>
<ul>
<li>model-ep002-loss3.245-val_loss3.612.h5</li>
</ul>
<p>&#x8BE5;&#x6A21;&#x578B;&#x5728;&#x7B2C; 2 &#x4E2A; epoch &#x4E2D;&#x7ED3;&#x675F;&#x65F6;&#x88AB;&#x4FDD;&#x5B58;&#xFF0C;&#x5728;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x7684;&#x635F;&#x5931;&#x4E3A; 3.245&#xFF0C;&#x5728;&#x5F00;&#x53D1;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x7684;&#x635F;&#x5931;&#x4E3A; 3.612&#xFF0C;&#x6BCF;&#x4E2A;&#x4EBA;&#x7684;&#x5177;&#x4F53;&#x7ED3;&#x679C;&#x4E0D;&#x540C;&#x3002;&#x5982;&#x679C;&#x4F60;&#x5728; AWS &#x4E2D;&#x8FD0;&#x884C;&#x4E0A;&#x8FF0;&#x793A;&#x4F8B;&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x6A21;&#x578B;&#x6587;&#x4EF6;&#x590D;&#x5236;&#x56DE;&#x4F60;&#x5F53;&#x524D;&#x7684;&#x5DE5;&#x4F5C;&#x6587;&#x4EF6;&#x5939;&#x3002;</p>
<p><strong>&#x8BC4;&#x4F30;&#x6A21;&#x578B;</strong></p>
<p>&#x6A21;&#x578B;&#x62DF;&#x5408;&#x4E4B;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5728;&#x7559;&#x51FA;&#x7684;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x8BC4;&#x4F30;&#x5B83;&#x7684;&#x9884;&#x6D4B;&#x6280;&#x80FD;&#x3002;</p>
<p>&#x4F7F;&#x6A21;&#x578B;&#x5BF9;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#xFF0C;&#x4F7F;&#x7528;&#x6807;&#x51C6;&#x4EE3;&#x4EF7;&#x51FD;&#x6570;&#x8BC4;&#x4F30;&#x9884;&#x6D4B;&#xFF0C;&#x4ECE;&#x800C;&#x8BC4;&#x4F30;&#x6A21;&#x578B;&#x3002;</p>
<p>&#x9996;&#x5148;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x4F7F;&#x7528;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x5BF9;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#x3002;&#x8F93;&#x5165;&#x5F00;&#x59CB;&#x63CF;&#x8FF0;&#x7684;&#x6807;&#x8BB0; &#x300E;startseq&#x300E;&#xFF0C;&#x751F;&#x6210;&#x4E00;&#x4E2A;&#x5355;&#x8BCD;&#xFF0C;&#x7136;&#x540E;&#x9012;&#x5F52;&#x5730;&#x7528;&#x751F;&#x6210;&#x5355;&#x8BCD;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x542F;&#x7528;&#x6A21;&#x578B;&#x76F4;&#x5230;&#x5E8F;&#x5217;&#x6807;&#x8BB0;&#x5230; &#x300E;endseq&#x300E;&#x6216;&#x8FBE;&#x5230;&#x6700;&#x5927;&#x63CF;&#x8FF0;&#x957F;&#x5EA6;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x7684; generate_desc() &#x51FD;&#x6570;&#x5B9E;&#x73B0;&#x8BE5;&#x884C;&#x4E3A;&#xFF0C;&#x5E76;&#x57FA;&#x4E8E;&#x7ED9;&#x5B9A;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x548C;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x7684;&#x51C6;&#x5907;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#x3002;&#x5B83;&#x542F;&#x7528; word_for_id() &#x51FD;&#x6570;&#x4EE5;&#x6620;&#x5C04;&#x6574;&#x6570;&#x9884;&#x6D4B;&#x81F3;&#x5355;&#x8BCD;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# map an integer to a word</span><br><span class="line">def word_for_id(integer, tokenizer):</span><br><span class="line">	for word, index in tokenizer.word_index.items():</span><br><span class="line">		if index == integer:</span><br><span class="line">			return word</span><br><span class="line">	return None</span><br><span class="line"></span><br><span class="line"># generate a description for an image</span><br><span class="line">def generate_desc(model, tokenizer, photo, max_length):</span><br><span class="line">	# seed the generation process</span><br><span class="line">	in_text = &apos;startseq&apos;</span><br><span class="line">	# iterate over the whole length of the sequence</span><br><span class="line">	for i in range(max_length):</span><br><span class="line">		# integer encode input sequence</span><br><span class="line">		sequence = tokenizer.texts_to_sequences([in_text])[0]</span><br><span class="line">		# pad input</span><br><span class="line">		sequence = pad_sequences([sequence], maxlen=max_length)</span><br><span class="line">		# predict next word</span><br><span class="line">		yhat = model.predict([photo,sequence], verbose=0)</span><br><span class="line">		# convert probability to integer</span><br><span class="line">		yhat = argmax(yhat)</span><br><span class="line">		# map integer to word</span><br><span class="line">		word = word_for_id(yhat, tokenizer)</span><br><span class="line">		# stop if we cannot map the word</span><br><span class="line">		if word is None:</span><br><span class="line">			break</span><br><span class="line">		# append as input for generating the next word</span><br><span class="line">		in_text += &apos; &apos; + word</span><br><span class="line">		# stop if we predict the end of the sequence</span><br><span class="line">		if word == &apos;endseq&apos;:</span><br><span class="line">			break</span><br><span class="line">	return in_text</span><br></pre></td></tr></table></figure>

<p>&#x6211;&#x4EEC;&#x5C06;&#x4E3A;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x548C;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x9884;&#x6D4B;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x7684; evaluate_model() &#x57FA;&#x4E8E;&#x7ED9;&#x5B9A;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6570;&#x636E;&#x96C6;&#x548C;&#x56FE;&#x50CF;&#x7279;&#x5F81;&#x8BC4;&#x4F30;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x3002;&#x6536;&#x96C6;&#x5B9E;&#x9645;&#x548C;&#x9884;&#x6D4B;&#x63CF;&#x8FF0;&#xFF0C;&#x4F7F;&#x7528;&#x8BED;&#x6599;&#x5E93; BLEU &#x503C;&#x5BF9;&#x5B83;&#x4EEC;&#x8FDB;&#x884C;&#x8BC4;&#x4F30;&#x3002;&#x8BED;&#x6599;&#x5E93; BLEU &#x503C;&#x603B;&#x7ED3;&#x4E86;&#x751F;&#x6210;&#x6587;&#x672C;&#x548C;&#x671F;&#x671B;&#x6587;&#x672C;&#x4E4B;&#x95F4;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# evaluate the skill of the model</span><br><span class="line">def evaluate_model(model, descriptions, photos, tokenizer, max_length):</span><br><span class="line">	actual, predicted = list(), list()</span><br><span class="line">	# step over the whole set</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		# generate description</span><br><span class="line">		yhat = generate_desc(model, tokenizer, photos[key], max_length)</span><br><span class="line">		# store actual and predicted</span><br><span class="line">		references = [d.split() for d in desc_list]</span><br><span class="line">		actual.append(references)</span><br><span class="line">		predicted.append(yhat.split())</span><br><span class="line">	# calculate BLEU score</span><br><span class="line">	print(&apos;BLEU-1: %f&apos; % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))</span><br><span class="line">	print(&apos;BLEU-2: %f&apos; % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))</span><br><span class="line">	print(&apos;BLEU-3: %f&apos; % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))</span><br><span class="line">	print(&apos;BLEU-4: %f&apos; % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))</span><br></pre></td></tr></table></figure>

<p>BLEU &#x503C;&#x7528;&#x4E8E;&#x5728;&#x6587;&#x672C;&#x7FFB;&#x8BD1;&#x4E2D;&#x8BC4;&#x4F30;&#x8BD1;&#x6587;&#x548C;&#x4E00;&#x6216;&#x591A;&#x4E2A;&#x53C2;&#x8003;&#x8BD1;&#x6587;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;</p>
<p>&#x8FD9;&#x91CC;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6BCF;&#x4E2A;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#x4E0E;&#x8BE5;&#x56FE;&#x50CF;&#x7684;&#x6240;&#x6709;&#x53C2;&#x8003;&#x63CF;&#x8FF0;&#x8FDB;&#x884C;&#x5BF9;&#x6BD4;&#xFF0C;&#x7136;&#x540E;&#x8BA1;&#x7B97; 1&#x3001;2&#x3001;3&#x3001;4 &#x7B49; n &#x5143;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x7684; BLEU &#x503C;&#x3002;</p>
<p>NLTK Python &#x5E93;&#x5728; corpus_bleu() &#x51FD;&#x6570;&#x4E2D;&#x5B9E;&#x73B0;&#x4E86; BLEU &#x503C;&#x8BA1;&#x7B97;&#x3002;&#x5206;&#x503C;&#x8D8A;&#x63A5;&#x8FD1; 1.0 &#x8D8A;&#x597D;&#xFF0C;&#x8D8A;&#x63A5;&#x8FD1; 0 &#x8D8A;&#x5DEE;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x7ED3;&#x5408;&#x524D;&#x9762;&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x90E8;&#x5206;&#x4E2D;&#x7684;&#x51FD;&#x6570;&#x3002;&#x9996;&#x5148;&#x52A0;&#x8F7D;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x6765;&#x51C6;&#x5907; Tokenizer&#xFF0C;&#x4EE5;&#x4F7F;&#x6211;&#x4EEC;&#x5C06;&#x751F;&#x6210;&#x5355;&#x8BCD;&#x7F16;&#x7801;&#x6210;&#x6A21;&#x578B;&#x7684;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;&#x4F7F;&#x7528;&#x6A21;&#x578B;&#x8BAD;&#x7EC3;&#x65F6;&#x4F7F;&#x7528;&#x7684;&#x7F16;&#x7801;&#x673A;&#x5236;&#x5BF9;&#x751F;&#x6210;&#x5355;&#x8BCD;&#x8FDB;&#x884C;&#x7F16;&#x7801;&#x975E;&#x5E38;&#x5173;&#x952E;&#x3002;</p>
<p>&#x7136;&#x540E;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x51FD;&#x6570;&#x52A0;&#x8F7D;&#x6D4B;&#x8BD5;&#x6570;&#x636E;&#x96C6;&#x3002;&#x5B8C;&#x6574;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;from numpy import argmax</span><br><span class="line">from pickle import load</span><br><span class="line">from keras.preprocessing.text import Tokenizer</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from keras.models import load_model</span><br><span class="line">from nltk.translate.bleu_score import corpus_bleu</span><br><span class="line"></span><br><span class="line"># load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># load a pre-defined list of photo identifiers</span><br><span class="line">def load_set(filename):</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	dataset = list()</span><br><span class="line">	# process line by line</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# skip empty lines</span><br><span class="line">		if len(line) &lt; 1:</span><br><span class="line">			continue</span><br><span class="line">		# get the image identifier</span><br><span class="line">		identifier = line.split(&apos;.&apos;)[0]</span><br><span class="line">		dataset.append(identifier)</span><br><span class="line">	return set(dataset)</span><br><span class="line"></span><br><span class="line"># load clean descriptions into memory</span><br><span class="line">def load_clean_descriptions(filename, dataset):</span><br><span class="line">	# load document</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	descriptions = dict()</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		# split id from description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# skip images not in the set</span><br><span class="line">		if image_id in dataset:</span><br><span class="line">			# create list</span><br><span class="line">			if image_id not in descriptions:</span><br><span class="line">				descriptions[image_id] = list()</span><br><span class="line">			# wrap description in tokens</span><br><span class="line">			desc = &apos;startseq &apos; + &apos; &apos;.join(image_desc) + &apos; endseq&apos;</span><br><span class="line">			# store</span><br><span class="line">			descriptions[image_id].append(desc)</span><br><span class="line">	return descriptions</span><br><span class="line"></span><br><span class="line"># load photo features</span><br><span class="line">def load_photo_features(filename, dataset):</span><br><span class="line">	# load all features</span><br><span class="line">	all_features = load(open(filename, &apos;rb&apos;))</span><br><span class="line">	# filter features</span><br><span class="line">	features = {k: all_features[k] for k in dataset}</span><br><span class="line">	return features</span><br><span class="line"></span><br><span class="line"># covert a dictionary of clean descriptions to a list of descriptions</span><br><span class="line">def to_lines(descriptions):</span><br><span class="line">	all_desc = list()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.append(d) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># fit a tokenizer given caption descriptions</span><br><span class="line">def create_tokenizer(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	tokenizer = Tokenizer()</span><br><span class="line">	tokenizer.fit_on_texts(lines)</span><br><span class="line">	return tokenizer</span><br><span class="line"></span><br><span class="line"># calculate the length of the description with the most words</span><br><span class="line">def max_length(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	return max(len(d.split()) for d in lines)</span><br><span class="line"></span><br><span class="line"># map an integer to a word</span><br><span class="line">def word_for_id(integer, tokenizer):</span><br><span class="line">	for word, index in tokenizer.word_index.items():</span><br><span class="line">		if index == integer:</span><br><span class="line">			return word</span><br><span class="line">	return None</span><br><span class="line"></span><br><span class="line"># generate a description for an image</span><br><span class="line">def generate_desc(model, tokenizer, photo, max_length):</span><br><span class="line">	# seed the generation process</span><br><span class="line">	in_text = &apos;startseq&apos;</span><br><span class="line">	# iterate over the whole length of the sequence</span><br><span class="line">	for i in range(max_length):</span><br><span class="line">		# integer encode input sequence</span><br><span class="line">		sequence = tokenizer.texts_to_sequences([in_text])[0]</span><br><span class="line">		# pad input</span><br><span class="line">		sequence = pad_sequences([sequence], maxlen=max_length)</span><br><span class="line">		# predict next word</span><br><span class="line">		yhat = model.predict([photo,sequence], verbose=0)</span><br><span class="line">		# convert probability to integer</span><br><span class="line">		yhat = argmax(yhat)</span><br><span class="line">		# map integer to word</span><br><span class="line">		word = word_for_id(yhat, tokenizer)</span><br><span class="line">		# stop if we cannot map the word</span><br><span class="line">		if word is None:</span><br><span class="line">			break</span><br><span class="line">		# append as input for generating the next word</span><br><span class="line">		in_text += &apos; &apos; + word</span><br><span class="line">		# stop if we predict the end of the sequence</span><br><span class="line">		if word == &apos;endseq&apos;:</span><br><span class="line">			break</span><br><span class="line">	return in_text</span><br><span class="line"></span><br><span class="line"># evaluate the skill of the model</span><br><span class="line">def evaluate_model(model, descriptions, photos, tokenizer, max_length):</span><br><span class="line">	actual, predicted = list(), list()</span><br><span class="line">	# step over the whole set</span><br><span class="line">	for key, desc_list in descriptions.items():</span><br><span class="line">		# generate description</span><br><span class="line">		yhat = generate_desc(model, tokenizer, photos[key], max_length)</span><br><span class="line">		# store actual and predicted</span><br><span class="line">		references = [d.split() for d in desc_list]</span><br><span class="line">		actual.append(references)</span><br><span class="line">		predicted.append(yhat.split())</span><br><span class="line">	# calculate BLEU score</span><br><span class="line">	print(&apos;BLEU-1: %f&apos; % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))</span><br><span class="line">	print(&apos;BLEU-2: %f&apos; % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))</span><br><span class="line">	print(&apos;BLEU-3: %f&apos; % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))</span><br><span class="line">	print(&apos;BLEU-4: %f&apos; % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))</span><br><span class="line"></span><br><span class="line"># prepare tokenizer on train set</span><br><span class="line"></span><br><span class="line"># load training dataset (6K)</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.trainImages.txt&apos;</span><br><span class="line">train = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(train))</span><br><span class="line"># descriptions</span><br><span class="line">train_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, train)</span><br><span class="line">print(&apos;Descriptions: train=%d&apos; % len(train_descriptions))</span><br><span class="line"># prepare tokenizer</span><br><span class="line">tokenizer = create_tokenizer(train_descriptions)</span><br><span class="line">vocab_size = len(tokenizer.word_index) + 1</span><br><span class="line">print(&apos;Vocabulary Size: %d&apos; % vocab_size)</span><br><span class="line"># determine the maximum sequence length</span><br><span class="line">max_length = max_length(train_descriptions)</span><br><span class="line">print(&apos;Description Length: %d&apos; % max_length)</span><br><span class="line"></span><br><span class="line"># prepare test set</span><br><span class="line"></span><br><span class="line"># load test set</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.testImages.txt&apos;</span><br><span class="line">test = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(test))</span><br><span class="line"># descriptions</span><br><span class="line">test_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, test)</span><br><span class="line">print(&apos;Descriptions: test=%d&apos; % len(test_descriptions))</span><br><span class="line"># photo features</span><br><span class="line">test_features = load_photo_features(&apos;features.pkl&apos;, test)</span><br><span class="line">print(&apos;Photos: test=%d&apos; % len(test_features))</span><br><span class="line"></span><br><span class="line"># load the model</span><br><span class="line">filename = &apos;model-ep002-loss3.245-val_loss3.612.h5&apos;</span><br><span class="line">model = load_model(filename)</span><br><span class="line"># evaluate model</span><br><span class="line">evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)</span><br></pre></td></tr></table></figure>

<p>&#x8FD0;&#x884C;&#x793A;&#x4F8B;&#x6253;&#x5370; BLEU &#x503C;&#x3002;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x770B;&#x5230; BLEU &#x503C;&#x5904;&#x4E8E;&#x8BE5;&#x95EE;&#x9898;&#x8F83;&#x4F18;&#x7684;&#x671F;&#x671B;&#x8303;&#x56F4;&#x5185;&#xFF0C;&#x4E14;&#x63A5;&#x8FD1;&#x6700;&#x4F18;&#x6C34;&#x5E73;&#x3002;&#x5E76;&#x4E14;&#x6211;&#x4EEC;&#x5E76;&#x6CA1;&#x6709;&#x5BF9;&#x9009;&#x62E9;&#x7684;&#x6A21;&#x578B;&#x914D;&#x7F6E;&#x8FDB;&#x884C;&#x7279;&#x522B;&#x7684;&#x4F18;&#x5316;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;BLEU-1: 0.579114</span><br><span class="line">BLEU-2: 0.344856</span><br><span class="line">BLEU-3: 0.252154</span><br><span class="line">BLEU-4: 0.131446</span><br></pre></td></tr></table></figure>

<p><strong>&#x751F;&#x6210;&#x65B0;&#x7684;&#x56FE;&#x50CF;&#x5B57;&#x5E55;</strong></p>
<p>&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x4E86;&#x89E3;&#x4E86;&#x5982;&#x4F55;&#x5F00;&#x53D1;&#x548C;&#x8BC4;&#x4F30;&#x5B57;&#x5E55;&#x751F;&#x6210;&#x6A21;&#x578B;&#xFF0C;&#x90A3;&#x4E48;&#x6211;&#x4EEC;&#x5982;&#x4F55;&#x4F7F;&#x7528;&#x5B83;&#x5462;&#xFF1F;</p>
<p>&#x6211;&#x4EEC;&#x9700;&#x8981;&#x6A21;&#x578B;&#x6587;&#x4EF6;&#x4E2D;&#x5168;&#x65B0;&#x7684;&#x56FE;&#x50CF;&#xFF0C;&#x8FD8;&#x9700;&#x8981; Tokenizer &#x7528;&#x4E8E;&#x5BF9;&#x6A21;&#x578B;&#x751F;&#x6210;&#x5355;&#x8BCD;&#x8FDB;&#x884C;&#x7F16;&#x7801;&#xFF0C;&#x751F;&#x6210;&#x5E8F;&#x5217;&#x548C;&#x5B9A;&#x4E49;&#x6A21;&#x578B;&#x65F6;&#x4F7F;&#x7528;&#x7684;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x6700;&#x5927;&#x957F;&#x5EA6;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5BF9;&#x6700;&#x5927;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x8FDB;&#x884C;&#x786C;&#x7F16;&#x7801;&#x3002;&#x6587;&#x672C;&#x7F16;&#x7801;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x5C31;&#x53EF;&#x4EE5;&#x521B;&#x5EFA; tokenizer&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x4FDD;&#x5B58;&#x81F3;&#x6587;&#x4EF6;&#xFF0C;&#x8FD9;&#x6837;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5728;&#x9700;&#x8981;&#x7684;&#x65F6;&#x5019;&#x5FEB;&#x901F;&#x52A0;&#x8F7D;&#xFF0C;&#x65E0;&#x9700;&#x6574;&#x4E2A; Flickr8K &#x6570;&#x636E;&#x96C6;&#x3002;&#x53E6;&#x4E00;&#x4E2A;&#x65B9;&#x6CD5;&#x662F;&#x4F7F;&#x7528;&#x6211;&#x4EEC;&#x81EA;&#x5DF1;&#x7684;&#x8BCD;&#x6C47;&#x6587;&#x4EF6;&#xFF0C;&#x5728;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x4E2D;&#x5C06;&#x5176;&#x6620;&#x5C04;&#x5230;&#x53D6;&#x6574;&#x51FD;&#x6570;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x6309;&#x7167;&#x4E4B;&#x524D;&#x7684;&#x65B9;&#x5F0F;&#x521B;&#x5EFA; Tokenizer&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x4FDD;&#x5B58;&#x4E3A; pickle &#x6587;&#x4EF6; tokenizer.pkl&#x3002;&#x5B8C;&#x6574;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;from keras.preprocessing.text import Tokenizer</span><br><span class="line">from pickle import dump</span><br><span class="line"></span><br><span class="line"># load doc into memory</span><br><span class="line">def load_doc(filename):</span><br><span class="line">	# open the file as read only</span><br><span class="line">	file = open(filename, &apos;r&apos;)</span><br><span class="line">	# read all text</span><br><span class="line">	text = file.read()</span><br><span class="line">	# close the file</span><br><span class="line">	file.close()</span><br><span class="line">	return text</span><br><span class="line"></span><br><span class="line"># load a pre-defined list of photo identifiers</span><br><span class="line">def load_set(filename):</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	dataset = list()</span><br><span class="line">	# process line by line</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# skip empty lines</span><br><span class="line">		if len(line) &lt; 1:</span><br><span class="line">			continue</span><br><span class="line">		# get the image identifier</span><br><span class="line">		identifier = line.split(&apos;.&apos;)[0]</span><br><span class="line">		dataset.append(identifier)</span><br><span class="line">	return set(dataset)</span><br><span class="line"></span><br><span class="line"># load clean descriptions into memory</span><br><span class="line">def load_clean_descriptions(filename, dataset):</span><br><span class="line">	# load document</span><br><span class="line">	doc = load_doc(filename)</span><br><span class="line">	descriptions = dict()</span><br><span class="line">	for line in doc.split(&apos;\n&apos;):</span><br><span class="line">		# split line by white space</span><br><span class="line">		tokens = line.split()</span><br><span class="line">		# split id from description</span><br><span class="line">		image_id, image_desc = tokens[0], tokens[1:]</span><br><span class="line">		# skip images not in the set</span><br><span class="line">		if image_id in dataset:</span><br><span class="line">			# create list</span><br><span class="line">			if image_id not in descriptions:</span><br><span class="line">				descriptions[image_id] = list()</span><br><span class="line">			# wrap description in tokens</span><br><span class="line">			desc = &apos;startseq &apos; + &apos; &apos;.join(image_desc) + &apos; endseq&apos;</span><br><span class="line">			# store</span><br><span class="line">			descriptions[image_id].append(desc)</span><br><span class="line">	return descriptions</span><br><span class="line"></span><br><span class="line"># covert a dictionary of clean descriptions to a list of descriptions</span><br><span class="line">def to_lines(descriptions):</span><br><span class="line">	all_desc = list()</span><br><span class="line">	for key in descriptions.keys():</span><br><span class="line">		[all_desc.append(d) for d in descriptions[key]]</span><br><span class="line">	return all_desc</span><br><span class="line"></span><br><span class="line"># fit a tokenizer given caption descriptions</span><br><span class="line">def create_tokenizer(descriptions):</span><br><span class="line">	lines = to_lines(descriptions)</span><br><span class="line">	tokenizer = Tokenizer()</span><br><span class="line">	tokenizer.fit_on_texts(lines)</span><br><span class="line">	return tokenizer</span><br><span class="line"></span><br><span class="line"># load training dataset (6K)</span><br><span class="line">filename = &apos;Flickr8k_text/Flickr_8k.trainImages.txt&apos;</span><br><span class="line">train = load_set(filename)</span><br><span class="line">print(&apos;Dataset: %d&apos; % len(train))</span><br><span class="line"># descriptions</span><br><span class="line">train_descriptions = load_clean_descriptions(&apos;descriptions.txt&apos;, train)</span><br><span class="line">print(&apos;Descriptions: train=%d&apos; % len(train_descriptions))</span><br><span class="line"># prepare tokenizer</span><br><span class="line">tokenizer = create_tokenizer(train_descriptions)</span><br><span class="line"># save the tokenizer</span><br><span class="line">dump(tokenizer, open(&apos;tokenizer.pkl&apos;, &apos;wb&apos;))</span><br></pre></td></tr></table></figure>

<p>&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5728;&#x9700;&#x8981;&#x7684;&#x65F6;&#x5019;&#x52A0;&#x8F7D; tokenizer&#xFF0C;&#x65E0;&#x9700;&#x52A0;&#x8F7D;&#x6574;&#x4E2A;&#x6807;&#x6CE8;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x3002;&#x4E0B;&#x9762;&#xFF0C;&#x6211;&#x4EEC;&#x6765;&#x4E3A;&#x4E00;&#x4E2A;&#x65B0;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#xFF0C;&#x4E0B;&#x9762;&#x8FD9;&#x5F20;&#x56FE;&#x662F;&#x6211;&#x4ECE; Flickr &#x4E2D;&#x968F;&#x673A;&#x9009;&#x7684;&#x4E00;&#x5F20;&#x56FE;&#x50CF;&#x3002;</p>
<p><img src="https://gitee.com/songjianzaina/juejin_p4/raw/master/img/e2660351b2541a03b38b86b84fb9436710408d5333c8d898caba78970ecd1f7e" alt>  </p>
<p><em>&#x6D77;&#x6EE9;&#x4E0A;&#x7684;&#x72D7;</em></p>
<p>&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528;&#x6A21;&#x578B;&#x4E3A;&#x5B83;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#x3002;&#x9996;&#x5148;&#x4E0B;&#x8F7D;&#x56FE;&#x50CF;&#xFF0C;&#x4FDD;&#x5B58;&#x81F3;&#x672C;&#x5730;&#x6587;&#x4EF6;&#x5939;&#xFF0C;&#x6587;&#x4EF6;&#x540D;&#x8BBE;&#x7F6E;&#x4E3A;&#x300C;example.jpg&#x300D;&#x3002;&#x7136;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x5FC5;&#x987B;&#x4ECE; tokenizer.pkl &#x4E2D;&#x52A0;&#x8F7D; Tokenizer&#xFF0C;&#x5B9A;&#x4E49;&#x751F;&#x6210;&#x5E8F;&#x5217;&#x7684;&#x6700;&#x5927;&#x957F;&#x5EA6;&#xFF0C;&#x5728;&#x5BF9;&#x8F93;&#x5165;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x65F6;&#x9700;&#x8981;&#x8BE5;&#x4FE1;&#x606F;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load the tokenizer</span><br><span class="line">tokenizer = load(open(&apos;tokenizer.pkl&apos;, &apos;rb&apos;))</span><br><span class="line"># pre-define the max sequence length (from training)</span><br><span class="line">max_length = 34</span><br></pre></td></tr></table></figure>

<p>&#x7136;&#x540E;&#x6211;&#x4EEC;&#x5FC5;&#x987B;&#x52A0;&#x8F7D;&#x6A21;&#x578B;&#xFF0C;&#x5982;&#x524D;&#x6240;&#x8FF0;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# load the model</span><br><span class="line">model = load_model(&apos;model-ep002-loss3.245-val_loss3.612.h5&apos;)</span><br></pre></td></tr></table></figure>

<p>&#x63A5;&#x4E0B;&#x6765;&#xFF0C;&#x6211;&#x4EEC;&#x5FC5;&#x987B;&#x52A0;&#x8F7D;&#x8981;&#x63CF;&#x8FF0;&#x548C;&#x63D0;&#x53D6;&#x7279;&#x5F81;&#x7684;&#x56FE;&#x50CF;&#x3002;</p>
<p>&#x91CD;&#x5B9A;&#x4E49;&#x8BE5;&#x6A21;&#x578B;&#x3001;&#x5411;&#x5176;&#x4E2D;&#x6DFB;&#x52A0; VGG-16 &#x6A21;&#x578B;&#xFF0C;&#x6216;&#x8005;&#x4F7F;&#x7528; VGG &#x6A21;&#x578B;&#x6765;&#x9884;&#x6D4B;&#x7279;&#x5F81;&#xFF0C;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x7279;&#x5F81;&#x4F5C;&#x4E3A;&#x73B0;&#x6709;&#x6A21;&#x578B;&#x7684;&#x8F93;&#x5165;&#x3002;&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528;&#x540E;&#x4E00;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x4F7F;&#x7528;&#x6570;&#x636E;&#x51C6;&#x5907;&#x9636;&#x6BB5;&#x6240;&#x7528;&#x7684; extract_features() &#x51FD;&#x6570;&#x7684;&#x4FEE;&#x6B63;&#x7248;&#x672C;&#xFF0C;&#x8BE5;&#x7248;&#x672C;&#x9002;&#x5408;&#x5904;&#x7406;&#x5355;&#x4E2A;&#x56FE;&#x50CF;&#x3002;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;# extract features from each photo in the directory</span><br><span class="line">def extract_features(filename):</span><br><span class="line">	# load the model</span><br><span class="line">	model = VGG16()</span><br><span class="line">	# re-structure the model</span><br><span class="line">	model.layers.pop()</span><br><span class="line">	model = Model(inputs=model.inputs, outputs=model.layers[-1].output)</span><br><span class="line">	# load the photo</span><br><span class="line">	image = load_img(filename, target_size=(224, 224))</span><br><span class="line">	# convert the image pixels to a numpy array</span><br><span class="line">	image = img_to_array(image)</span><br><span class="line">	# reshape data for the model</span><br><span class="line">	image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line">	# prepare the image for the VGG model</span><br><span class="line">	image = preprocess_input(image)</span><br><span class="line">	# get features</span><br><span class="line">	feature = model.predict(image, verbose=0)</span><br><span class="line">	return feature</span><br><span class="line"></span><br><span class="line"># load and prepare the photograph</span><br><span class="line">photo = extract_features(&apos;example.jpg&apos;)</span><br></pre></td></tr></table></figure>

<p>&#x4E4B;&#x540E;&#x4F7F;&#x7528;&#x8BC4;&#x4F30;&#x6A21;&#x578B;&#x5B9A;&#x4E49;&#x7684; generate_desc() &#x51FD;&#x6570;&#x751F;&#x6210;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x3002;&#x4E3A;&#x5355;&#x4E2A;&#x5168;&#x65B0;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x63CF;&#x8FF0;&#x7684;&#x5B8C;&#x6574;&#x793A;&#x4F8B;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;from pickle import load</span><br><span class="line">from numpy import argmax</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from keras.applications.vgg16 import VGG16</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.applications.vgg16 import preprocess_input</span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.models import load_model</span><br><span class="line"></span><br><span class="line"># extract features from each photo in the directory</span><br><span class="line">def extract_features(filename):</span><br><span class="line">	# load the model</span><br><span class="line">	model = VGG16()</span><br><span class="line">	# re-structure the model</span><br><span class="line">	model.layers.pop()</span><br><span class="line">	model = Model(inputs=model.inputs, outputs=model.layers[-1].output)</span><br><span class="line">	# load the photo</span><br><span class="line">	image = load_img(filename, target_size=(224, 224))</span><br><span class="line">	# convert the image pixels to a numpy array</span><br><span class="line">	image = img_to_array(image)</span><br><span class="line">	# reshape data for the model</span><br><span class="line">	image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line">	# prepare the image for the VGG model</span><br><span class="line">	image = preprocess_input(image)</span><br><span class="line">	# get features</span><br><span class="line">	feature = model.predict(image, verbose=0)</span><br><span class="line">	return feature</span><br><span class="line"></span><br><span class="line"># map an integer to a word</span><br><span class="line">def word_for_id(integer, tokenizer):</span><br><span class="line">	for word, index in tokenizer.word_index.items():</span><br><span class="line">		if index == integer:</span><br><span class="line">			return word</span><br><span class="line">	return None</span><br><span class="line"></span><br><span class="line"># generate a description for an image</span><br><span class="line">def generate_desc(model, tokenizer, photo, max_length):</span><br><span class="line">	# seed the generation process</span><br><span class="line">	in_text = &apos;startseq&apos;</span><br><span class="line">	# iterate over the whole length of the sequence</span><br><span class="line">	for i in range(max_length):</span><br><span class="line">		# integer encode input sequence</span><br><span class="line">		sequence = tokenizer.texts_to_sequences([in_text])[0]</span><br><span class="line">		# pad input</span><br><span class="line">		sequence = pad_sequences([sequence], maxlen=max_length)</span><br><span class="line">		# predict next word</span><br><span class="line">		yhat = model.predict([photo,sequence], verbose=0)</span><br><span class="line">		# convert probability to integer</span><br><span class="line">		yhat = argmax(yhat)</span><br><span class="line">		# map integer to word</span><br><span class="line">		word = word_for_id(yhat, tokenizer)</span><br><span class="line">		# stop if we cannot map the word</span><br><span class="line">		if word is None:</span><br><span class="line">			break</span><br><span class="line">		# append as input for generating the next word</span><br><span class="line">		in_text += &apos; &apos; + word</span><br><span class="line">		# stop if we predict the end of the sequence</span><br><span class="line">		if word == &apos;endseq&apos;:</span><br><span class="line">			break</span><br><span class="line">	return in_text</span><br><span class="line"></span><br><span class="line"># load the tokenizer</span><br><span class="line">tokenizer = load(open(&apos;tokenizer.pkl&apos;, &apos;rb&apos;))</span><br><span class="line"># pre-define the max sequence length (from training)</span><br><span class="line">max_length = 34</span><br><span class="line"># load the model</span><br><span class="line">model = load_model(&apos;model-ep002-loss3.245-val_loss3.612.h5&apos;)</span><br><span class="line"># load and prepare the photograph</span><br><span class="line">photo = extract_features(&apos;example.jpg&apos;)</span><br><span class="line"># generate description</span><br><span class="line">description = generate_desc(model, tokenizer, photo, max_length)</span><br><span class="line">print(description)</span><br></pre></td></tr></table></figure>

<p>&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x751F;&#x6210;&#x7684;&#x63CF;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x590D;&#x5236;&#x4EE3;&#x7801;startseq dog is running across the beach endseq</span><br></pre></td></tr></table></figure>

<p>&#x79FB;&#x9664;&#x5F00;&#x59CB;&#x548C;&#x7ED3;&#x675F;&#x7684;&#x6807;&#x8BB0;&#xFF0C;&#x6216;&#x8BB8;&#x8FD9;&#x5C31;&#x662F;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x6A21;&#x578B;&#x751F;&#x6210;&#x7684;&#x8BED;&#x53E5;&#x3002;&#x81F3;&#x6B64;&#xFF0C;&#x6211;&#x4EEC;&#x73B0;&#x5728;&#x5DF2;&#x7ECF;&#x5B8C;&#x6574;&#x5730;&#x4F7F;&#x7528;&#x6A21;&#x578B;&#x4E3A;&#x56FE;&#x50CF;&#x751F;&#x6210;&#x6587;&#x672C;&#x63CF;&#x8FF0;&#xFF0C;&#x867D;&#x7136;&#x8FD9;&#x4E00;&#x5B9E;&#x73B0;&#x975E;&#x5E38;&#x57FA;&#x7840;&#x4E0E;&#x7B80;&#x5355;&#xFF0C;&#x4F46;&#x5B83;&#x662F;&#x6211;&#x4EEC;&#x7EE7;&#x7EED;&#x5B66;&#x4E60;&#x5F3A;&#x5927;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6A21;&#x578B;&#x7684;&#x57FA;&#x7840;&#x3002;&#x6211;&#x4EEC;&#x4E5F;&#x5E0C;&#x671B;&#x672C;&#x6587;&#x80FD;&#x5E26;&#x9886;&#x7ED9;&#x4E3A;&#x8BFB;&#x8005;&#x5B9E;&#x64CD;&#x5730;&#x7406;&#x89E3;&#x56FE;&#x50CF;&#x63CF;&#x8FF0;&#x6A21;&#x578B;&#x3002;</p>
<p><em>&#x539F;&#x6587;&#x94FE;&#x63A5;&#xFF1A;<a href="/external_links/9ab2937b9ad407a81ae5110de166ac45.html" target="blank" rel="noopener">machinelearningmastery.com/develop-a-d&#x2026;</a></em></p>
<p><strong>&#x672C;&#x6587;&#x8F6C;&#x8F7D;&#x81EA;:</strong> <a href="/external_links/b6fd82d9d04628bdeddaeb1b11e97c46.html" target="blank" rel="noopener">&#x6398;&#x91D1;</a></p>
<p><em><a href="https://dev.newban.cn/">&#x5F00;&#x53D1;&#x8005;&#x535A;&#x5BA2; &#x2013; &#x548C;&#x5F00;&#x53D1;&#x76F8;&#x5173;&#x7684; &#x8FD9;&#x91CC;&#x5168;&#x90FD;&#x6709;</a></em></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="tags/Python/" rel="tag"># Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="6844903521142177799.html" rel="next" title="使用 Hyperledger Composer 创建强大的区">
                <i class="fa fa-chevron-left"></i> 使用 Hyperledger Composer 创建强大的区
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="6844903521158971405.html" rel="prev" title="你必须知道的HTTP基本概念">
                你必须知道的HTTP基本概念 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


    <!-- 有瓣音频文章内嵌广告 -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-2626449904708114" data-ad-slot="6145016388"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

    <div class="post-spread">
      
    </div>
  </div>
  <!-- 有瓣音频信息流广告 -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-6t+ed+2i-1n-4w" data-ad-client="ca-pub-2626449904708114" data-ad-slot="6455528644"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">开发者博客</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://dev.newban.cn/archives/">
              
                  <span class="site-state-item-count">3990</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1304</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">开发者博客</span>
  <div>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
  </div>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




  <script type="text/javascript" async src="js/src/mermaid.min.js"></script>
  <script>
  if (window.mermaid) {
        var mermaid_config = {
            startOnLoad: true,
            theme: 'default',
            flowchart:{
                useMaxWidth: false,
                htmlLabels: true
            }                
        }
        mermaid.initialize(mermaid_config);
  }
  </script>
  

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
